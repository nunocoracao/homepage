[{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/tags/entrepreneurship/","section":"Tags","summary":"","title":"Entrepreneurship"},{"content":"As a Product Manager, more often than not, I notice people mixing up ideation and execution phases in discussions. Both when evaluating new initiatives to bet on and when evaluating what is happening in their industry. Knowing the difference between these two concepts, how to manage them, and what importance they should have in different occasions is key for someone working on building ideas (e.g. Product), and can, ultimately, make a company/team succeed or fail. This confusion isn\u0026rsquo;t just a minor inconvenience; it\u0026rsquo;s a significant obstacle to innovation and strategic execution.\nIdeas VS Execution # What is an Idea? # An idea is a concept or a vision. It’s the initial spark of creativity that suggests a new way of doing something, solving a problem, or addressing a need. Ideas are abundant and can range from the mundane to the revolutionary. However, ideas by themselves are intangible and hold potential rather than value.\nWhat is Execution? # Execution is the process of taking an idea and turning it into reality. It involves planning, development, and implementation. Execution is where strategy, skill, and effort come into play to transform a concept into a product, service, or result. Unlike ideas, execution is tangible, measurable, and ultimately, what delivers value.\nComparison and Common Misunderstandings # The key difference lies in their nature; ideas are about \u0026ldquo;what\u0026rdquo; and \u0026ldquo;why\u0026rdquo;, while execution is about \u0026ldquo;how\u0026rdquo; and \u0026ldquo;when\u0026rdquo;. People often confuse excitement and enthusiasm for an idea with the practicalities and challenges of executing it. This confusion can lead to unrealistic expectations, misallocation of resources, and disappointment.\nExamples # Messaging Apps # Consider the concept of instant messaging. The idea is simple: enable real-time text-based communication between users. However, the execution varies widely, from WhatsApp\u0026rsquo;s focus on simplicity and security to Slack\u0026rsquo;s emphasis on integrating work communication and tools. The idea might be similar, but execution defines success and market differentiation.\nSocial Networks # Social networking platforms all stem from one idea: connecting people online. Yet, the execution of this idea has led to vastly different platforms. LinkedIn focuses on professional networking, while Instagram emphasizes visual content. Though rooted in the same basic concept, their executions cater to different needs and behaviors.\nMobile Phones # The transition from feature phones to smartphones is a prime example of execution taking precedence over the idea. The idea of a mobile phone remained consistent, but the execution in terms of software capabilities, internet connectivity, and touch interfaces revolutionized the industry.\nAugmented Reality (AR) # AR technology offers a broad idea: overlaying digital information onto the real world. However, its execution ranges from Snapchat filters and Pokémon GO to complex industrial and educational applications, demonstrating how diverse execution can fulfill different visions.\nProblems Arising from Confusion # Lack of Alignment # When teams conflate ideas with execution, there\u0026rsquo;s a risk of misalignment on goals, expectations, and measures of success. This misalignment can lead to wasted efforts and resources as teams may pursue the wrong objectives.\nManaging Expectations Becomes Challenging # Stakeholders might fall in love with the idea without fully understanding the complexity of its execution. This discrepancy can lead to frustration when the reality of the time, cost, and effort required to realize the idea becomes apparent.\nComparisons at the Wrong Level # Comparing high-level ideas without considering the nuances of execution leads to oversimplification. It ignores the subtleties that can make or break the success of a project. Different executions of the same idea can lead to vastly different outcomes, and one approach may serve multiple ideas.\nWhy Distinguishing Between Ideas and Execution Matters # The Value Equation # While ideas are the seed, execution is the sunlight, water, and soil that allow the seed to grow. An average idea with excellent execution can outperform a brilliant idea with poor execution. Recognizing this shifts the focus towards how an idea is brought to life, rather than the idea itself.\nFacilitating the Building Process # Clear differentiation helps in managing the building process with stakeholders. It clarifies expectations, aligns efforts, and focuses on the practical steps needed to realize a vision. This approach fosters a culture of accountability and pragmatism.\nUnderstanding the Competitive Landscape # In analyzing competitors and market forces, understanding that the execution behind their products or services is what sets them apart allows for more strategic planning. It highlights the importance of innovation not just in what you do, but how you do it.\nConclusion # As we navigate the complex landscape of product development and innovation, distinguishing between ideas and their execution becomes not just beneficial, but essential. It informs our strategies, aligns our teams, and ultimately, determines our success in the market. By recognizing the value of execution and dedicating the necessary resources and effort to it, we can transform even the simplest ideas into remarkable realities. This mindset is what separates the dreamers from the doers, and in the world of business, it\u0026rsquo;s the doers who lead the way.\n","date":"2024-03-15","externalUrl":null,"permalink":"/posts/202403-execution-is-king/","section":"Posts","summary":"As a Product Manager, more often than not, I notice people mixing up ideation and execution phases in discussions. Both when evaluating new initiatives to bet on and when evaluating what is happening in their industry.","title":"Execution is King"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/tags/innovation/","section":"Tags","summary":"","title":"Innovation"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/","section":"N9O","summary":"","title":"N9O"},{"content":" Staff Product Manager ","date":"2024-03-15","externalUrl":null,"permalink":"/authors/nunocoracao/","section":"Authors","summary":" Staff Product Manager ","title":"Nuno Coração"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/categories/opinion/","section":"Categories","summary":"","title":"Opinion"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/categories/product/","section":"Categories","summary":"","title":"Product"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/categories/strategy/","section":"Categories","summary":"","title":"Strategy"},{"content":"","date":"2024-03-15","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"2024-02-06","externalUrl":null,"permalink":"/tags/blog/","section":"Tags","summary":"","title":"Blog"},{"content":"","date":"2024-02-06","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"","date":"2024-02-06","externalUrl":null,"permalink":"/categories/external/","section":"Categories","summary":"","title":"External"},{"content":"","date":"2024-02-06","externalUrl":null,"permalink":"/tags/release/","section":"Tags","summary":"","title":"Release"},{"content":"In May 2023, Docker announced the beta release of docker init, a new command-line interface (CLI) tool in Docker Desktop designed to streamline the Docker setup process for various types of applications and help users containerize their existing projects. We’re now excited to announce the general availability of docker init, with support for multiple languages and stacks, making it simpler than ever to containerize your applications.\nWhat is docker init? # Initially released in its beta form in Docker 4.18, docker init has undergone several enhancements. docker initis a command-line utility that aids in the initialization of Docker resources within a project. It automatically generates Dockerfiles, Compose files, and.dockerignore files based on the nature of the project, significantly reducing the setup time and complexity associated with Docker configurations.\nThe initial beta release of init came with support only for Go and generic projects. The latest version, available in Docker Desktop 4.27, supports Go, Python, Node.js, Rust, ASP.NET, PHP, and Java.\nHow to use docker init # Using docker init is straightforward and involves a few simple steps. Start by navigating to your project directory where you want the Docker assets to be initialized. In the terminal, execute the docker init command. This command initiates the tool and prepares it to analyze your project (Figure 1).\ndocker init will scan your project and ask you to confirm and choose the template that best suits your application. Once you select the template, docker init asks you for some project-specific information, automatically generating the necessary Docker resources for your project (Figure 2).\nThis step includes creating a Dockerfile and a Compose file tailored to the language and framework of your choice, as well as other relevant files. The last step is to run docker-compose up to start your newly containerized project.\nWhy use docker init? # The docker init tool simplifies the process of dockerization, making it accessible even to those new to Docker. It eliminates the need to manually write Dockerfiles and other configuration files from scratch, saving time and reducing the potential for errors. With its template-based approach, docker init ensures that the Docker setup is optimized for the specific type of application you are working on and that your project will follow the industry’s best practices.\nConclusion # The general availability of docker init offers an efficient and user-friendly way to integrate Docker into your projects. Whether you’re a seasoned Docker user or new to containerization, docker init is set to enhance your development workflow.\nNote: this post was originally posted externally please go to Docker\u0026rsquo;s blog to read the full post ","date":"2024-02-06","externalUrl":"https://www.docker.com/blog/streamline-dockerization-with-docker-init-ga/","permalink":"/posts/202402-docker-init-ga/","section":"Posts","summary":"Initialize Dockerfiles and Compose files with a single CLI command","title":"Streamline Dockerization with Docker Init GA"},{"content":"","date":"2024-01-15","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI"},{"content":"","date":"2024-01-15","externalUrl":null,"permalink":"/categories/ai/","section":"Categories","summary":"","title":"AI"},{"content":" “We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.”\n— Roy Amara 1\nIt’s unquestionable the impact AI had in the world in the last year. Back in October 2022, I wrote about the fast-paced evolution of AI and how everything that was possible at the time felt like magic. Given everything that happened since then, I think it deserves a follow-up.\nThe New Artificial Intelligence Hype 18 mins\u0026middot; loading Nuno Coração Product Strategy Technology Stable Diffusion Midjourney Dall-E AI Machine Learning Last time I focused on the technology itself, what advancements were key to enabling GPTs, and made some predictions about the future. Some spot on, some maybe not. One year ago, the main topic was the sudden rise of AI applications since the creation of transformers. Since then, the speed of innovation hasn’t decreased one bit, quite the opposite. Investment in the area has grown massively in the last year, and everyone is thinking about how to leverage AI and be the one to win the ongoing race.\nWhere we are # The AI landscape has dramatically evolved over the last year, marked by significant investments, technological advancements, and a surge in AI applications across various sectors. At this stage, the entire tech industry is in an aggressive race to figure out these new technologies and create/integrate them as soon as possible.\nOpenAI and Microsoft # OpenAI\u0026rsquo;s collaboration with Microsoft, marked by substantial investments, has led to groundbreaking developments like GPT-4, the OpenAI API, and the GPT Store. The financial infusion from Microsoft ( $13B) not only catalyzes technological advancements but also opens new revenue ( estimated at $1.3B with OpenAI valued at $86.00B) streams through these AI products. Sam Altman\u0026rsquo;s modest lifestyle, despite OpenAI\u0026rsquo;s immense success, symbolizes a culture focused more on technological advancement and ethical considerations than on lavish lifestyles. This partnership is reshaping the industry, setting new standards for AI capabilities and applications in various sectors, from customer service to software development.\nNvidia # Nvidia\u0026rsquo;s role as the leading hardware provider for AI models is pivotal. The surge in their stock price reflects the critical demand for their GPUs, necessary for training and running AI models. By providing the hardware backbone for AI, Nvidia is enabling the rapid expansion of AI applications across industries, from healthcare to finance, ensuring that the infrastructure for AI is robust and scalable.\nNvidia stock price in the last 5 years - almost 500% growth Google # Google\u0026rsquo;s launch of its AI models signifies its determination to remain at the forefront of technological innovation. These models, rivaling those of OpenAI, indicate a competitive market that drives rapid advancements in AI. Google\u0026rsquo;s entry also ensures that AI technology continues to evolve, pushing the boundaries of machine learning and data processing capabilities. This competition is crucial for the industry, ensuring a continuous improvement in AI technologies.\nAmazon # Amazon has made significant strides in AI, marked by its investments in Anthropic, the launch of Bedrock, and the development of Titan models. Anthropic\u0026rsquo;s focus on AI safety and alignment represents a forward-thinking approach to AI development, aligning with Amazon\u0026rsquo;s commitment to responsible AI. The Bedrock model, a large-scale language model, extends Amazon\u0026rsquo;s capabilities in natural language processing, enhancing customer experience and operational efficiency. Titan models, known for their advanced computational power, further Amazon\u0026rsquo;s prowess in deep learning, enabling more robust and sophisticated AI applications. These investments and developments underscore Amazon\u0026rsquo;s dedication to leading in AI innovation and application, significantly impacting various aspects of technology and business.\nMeta # Meta\u0026rsquo;s contribution to open-source AI models, coupled with technologies like Ollama, is a game-changer. By enabling the local operation of powerful AI models, these initiatives democratize AI, allowing smaller companies and individual developers to run, train, and host their models or leverage open-source ones. This shift reduces reliance on big tech companies, fostering innovation and creativity across the board, and giving rise to a new wave of AI-driven applications and startups. Meta is not only interested in OSS and seems to be coming for OpenAI with the latest news on building a chip arsenal.\nHugging Face and OSS # Hugging Face has emerged as a pivotal force in the open-source AI movement, championing accessibility and collaborative innovation. It\u0026rsquo;s a platform where groundbreaking open-source models like Mistral and Orca are made readily available, breaking down traditional barriers in AI development. These models exemplify the power of open-source AI, offering advanced capabilities similar to those of major tech firms. This revolution allows a diverse array of developers, researchers, and smaller organizations not just to access cutting-edge AI technology, but to actively contribute to its evolution. Beyond mere access, Hugging Face fosters a dynamic environment of shared progress and innovation. By providing access to such powerful tools, Hugging Face and its counterparts are accelerating the pace of AI development, allowing a wider community to drive advancements and apply AI in novel and impactful ways. This inclusive model of growth ensures a rapid evolution of AI technologies, broadening the scope and scale of their application far beyond what proprietary models could achieve alone.\nImage and Video Magic # Tools like RunwayML, Midjourney, Pika, Kaiber, Invoke, and so many others are transforming the creative industries. By enabling the generation of high-quality images and videos, these platforms are altering how artists, designers, and media professionals work, making complex visual creations more accessible and efficient. This revolution in digital art and media production is enhancing creativity and changing the landscape of advertising, entertainment, and visual communications.\nStartups in the AI Race # The AI landscape is bustling with startups that are rapidly gaining traction (and investment), each bringing unique innovations to the field:\nCohere: Specializes in natural language understanding and generation, offering tools that enhance AI\u0026rsquo;s ability to interpret and respond to human language. Perplexity AI: This startup is making strides in AI-powered search and information retrieval. Perplexity AI focuses on enhancing the precision and relevance of search results. Anthropic: Focuses on AI safety and alignment, working towards creating AI systems that are reliable and align with human values. Mistral: Known for its contributions in the field of AI, particularly in enhancing the efficiency and effectiveness of AI models. Stability: Develops AI solutions aimed at stabilizing and improving the performance of various AI applications. Inflection: Works on refining AI\u0026rsquo;s decision-making processes, making it more context-aware and responsive to real-world scenarios. Adept: Concentrates on AI learning and adaptation, ensuring AI systems can learn and evolve with changing environments. Contextual AI: Aims at enhancing the contextual understanding of AI, allowing for more nuanced and accurate interactions in various applications. These startups are not only contributing to the advancement of AI technology, but also shaping the future of how AI is integrated and utilized across different industries.\nRAG Applications # The increasing use of Retrieval-Augmented Generation (RAG) techniques marks a significant evolution in AI applications. By combining language models with external knowledge sources, RAG enables more sophisticated and contextually aware AI systems. This has led to a rise in innovative applications across sectors, improving the capabilities of AI in areas like customer interaction, content curation, and decision support systems. The widespread adoption of RAG is a testament to the growing sophistication and practical utility of AI in real-world scenarios. The most used tools in this space are llamaindex and langchain.\nConcerns # The rapid advancement of AI technology has brought forth several concerns that are shaping the current discourse in the industry.\nLack of Knowledge, AGI, and Alignment # The understanding of how neural networks, particularly those powering advanced AI systems, operate is still limited. This lack of comprehensive knowledge feeds into the fears surrounding the achievement of Artificial General Intelligence (AGI) — an AI with the ability to understand, learn, and apply its intelligence to a wide range of problems, akin to human intelligence. There\u0026rsquo;s a concern that AGI could lead to unforeseen and potentially catastrophic outcomes. For example, former OpenAI employee Paul Christiano estimates a 10-20% chance of an AI takeover leading to a significant human catastrophe. This aligns with the broader concern of AI alignment, ensuring AI objectives are harmonizing with human values and intentions. Christiano\u0026rsquo;s comments emphasize the gravity of these concerns, as he highlights the potential risks once AI surpasses human-level intelligence​​.\nCopyright Issues # As AI models are often trained on publicly available data, including content from the internet, copyright concerns, particularly regarding artistic work, have emerged. These issues arise from the AI\u0026rsquo;s ability to generate content that closely resembles original human creations, blurring the lines of authorship and intellectual property rights. The ensuing legal battles and discussions underscore the need for clear guidelines and regulations in this area.\nBusiness Model and Sustainability # Despite the substantial revenues generated by companies like OpenAI, the path to profitability remains unclear. The high operational expenses associated with running sophisticated AI models, such as ChatGPT, pose a significant financial challenge. For instance, it\u0026rsquo;s reported that OpenAI may face difficulties sustaining its operations due to the high daily costs of running ChatGPT (estimated between $100K to $700K per day). This situation highlights a broader concern in the AI industry: while there\u0026rsquo;s a race to advance and deploy AI technologies, the financial sustainability of these endeavors is not always certain. The industry might be heading towards a \u0026lsquo;race to the bottom\u0026rsquo;, where the pursuit of technological advancement overshadows the economic viability.\nOther concerns include ethical considerations, such as biases in AI algorithms and the potential misuse of AI technologies. Privacy issues are also paramount, as AI systems often require large amounts of data, which might include sensitive personal information. All of these illustrate the complex landscape of AI development and deployment, where the excitement and potential of technological breakthroughs are tempered by significant ethical, legal, and financial challenges.\nWhat now? # Amara’s law, coined by Roy Amara, a respected researcher and futurist, states:\n“We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.”\n— Roy Amara 2\nThis perspective offers a nuanced framework for understanding the evolution of AI — separating the immediate hype from the long-term reality.\nAmara\u0026rsquo;s Law - orange actual benefit, blue perceived benefit Short-Term Perspectives # In the short term, the excitement surrounding AI\u0026rsquo;s capabilities can typically lead to inflated expectations. As we\u0026rsquo;ve seen with advancements in models like GPT-4, the promise of immediate transformation in industries like customer service, content creation, and even healthcare is compelling. However, this enthusiasm can sometimes overshadow the current limitations of AI, including issues of reliability, ethical dilemmas, and the technology\u0026rsquo;s nascent state. We tend to anticipate rapid and sweeping changes, but the reality is often more complex and gradual. The immediate future of AI is more about incremental improvements and finding effective ways to integrate these technologies into existing systems responsibly and ethically.\nLong-Term Projections # Looking at the long-term impact of AI, we might be underestimating its potential transformative effects. The gradual refinement and integration of AI technologies could lead to profound changes in how we live and work. Over time, AI could reshape entire industries, revolutionize scientific research, and alter the fabric of social interactions. The potential for AI to address complex global challenges, enhance human capabilities, and even redefine aspects of human experience is immense. Eventually, AI could be the cornerstone of major breakthroughs in fields like environmental conservation, medicine, and space exploration, shaping a currently hard future to fully envision.\nConclusion # Amara\u0026rsquo;s law aptly captures the dichotomy in our perception of technological advancements like AI. As we navigate the short-term challenges and excitement, it\u0026rsquo;s crucial to maintain a balanced perspective, acknowledging both the current limitations and the vast potential. The journey of AI is a marathon, not a sprint. It requires careful consideration, ethical stewardship, and a commitment to ongoing research and development. As we stand now, the future of AI holds both great promise and significant responsibility. It\u0026rsquo;s up to us to steer this technology towards outcomes that benefit humanity while mitigating risks and ensuring equitable and ethical use.\nhttps://deviq.com/laws/amaras-law\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://deviq.com/laws/amaras-law\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-01-15","externalUrl":null,"permalink":"/posts/202401-evolution-ai/","section":"Posts","summary":"We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.","title":"Evolution of AI and Amara's Law"},{"content":"","date":"2024-01-15","externalUrl":null,"permalink":"/tags/future/","section":"Tags","summary":"","title":"Future"},{"content":"","date":"2024-01-15","externalUrl":null,"permalink":"/categories/future/","section":"Categories","summary":"","title":"Future"},{"content":"","date":"2024-01-15","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"2024-01-15","externalUrl":null,"permalink":"/tags/technology/","section":"Tags","summary":"","title":"Technology"},{"content":"","date":"2024-01-15","externalUrl":null,"permalink":"/series/the-new-ai-hype/","section":"Series","summary":"","title":"The New AI Hype"},{"content":" Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","externalUrl":null,"permalink":"/suggestions/no-rules-rules-copy-2/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":" Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","externalUrl":null,"permalink":"/suggestions/no-rules-rules-copy-3/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":" Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","externalUrl":null,"permalink":"/suggestions/no-rules-rules-copy/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":" Books # Design of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nDesign of Everyday Things # asdasd\nZero to One # asdasd\nLean Startup # asdasd\nZero to One # asdasd\nArticles # blabla # asdasd\nOnline Publications # Stratechery # asdasd\nHBR # asdasd\nPodcasts # bla bal # ","date":"2023-11-16","externalUrl":null,"permalink":"/suggestions/no-rules-rules/","section":"Recs","summary":"tbd","title":"No Rules Rules"},{"content":"tbd\n","date":"2023-11-16","externalUrl":null,"permalink":"/suggestions/","section":"Recs","summary":"tbd","title":"Recs"},{"content":"","date":"2023-10-04","externalUrl":null,"permalink":"/tags/blowfish/","section":"Tags","summary":"","title":"Blowfish"},{"content":"","date":"2023-10-04","externalUrl":null,"permalink":"/categories/blowfish/","section":"Categories","summary":"","title":"Blowfish"},{"content":"Just one year ago, I created Blowfish, a Hugo theme crafted to build my unique vision for my personal homepage. I also decided to make it an open-source project. Fast-forward to today, and Blowfish has transformed into a thriving open-source project with over 600 stars on GitHub and a user base of hundreds. In this tutorial, I’ll show you how to get started and have your website running in a couple of minutes.\nnunocoracao/blowfish Personal Website \u0026amp; Blog Theme for Hugo HTML 617 198 TL;DR # The goal of this guide is to walk a newcomer to Hugo on how to install, manage, and publish your own website. The final version of the code is available in this repo - for those that would like to jump to the end.\nThe visual style is just one of the many possibilities available in Blowfish. Users are encouraged to check the documentation page and learn how to customize the theme to their needs. Additionally, there are already great examples of the theme from other users available for inspiration. Blowfish also offers several extra features in the form of shortcodes available out of the box in the theme - check them out here and get inspired.\nSetup your environment # Let’s begin by installing all the tools you need. This guide will cover the steps for Mac so these instructions might not apply to your hardware and OS. If you are on Windows or Linux, please consult the guides on how to install Hugo, and GitHub’s CLI for your OS.\nAnyway, if you are using MacOS let’s install brew - a package manager for mac - as that will help installing and managing the other tools.\n/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Once brew is installed let’s install Git, Hugo and GitHub’s CLI.\nbrew install git brew install hugo brew install gh Create a folder for your code and open a terminal session into it (I chose blowfish-tutorial in the commands below, feel free to call the folder whatever you want).\nmkdir blowfish-tutorial cd blowfish-tutorial Once inside the folder, the next step is to initialize your local git repo.\ngit init -b main Now, let’s create and sync the local repo to a GitHub repo so that your code is stored remotely.\ngh auth login gh repo create git push --set-upstream origin main Check the image below for the options I selected for this guide, again feel free to change names and description to your use-case.\nFinally, create a .gitignore file which allows you to exclude certain files from your repo automatically. I would start with something like the example below.\n#others node_modules .hugo_build.lock # OS generated files .DS_Store .DS_Store? ._* .Spotlight-V100 .Trashes # Hugo public The last step is to save all the changes to the repo.\ngit add . git commit -m “initial commit” git push Create site and configure it # With all the tools ready, creating and configuring your site will be easy. Still within the folder you created in the last section, let’s create an empty Hugo website (with no theme).\nhugo new site --force . Once the scaffolding finishes, try the command below to run your page. Open a browser on https://localhost:1313 to see your site…\nhugo server Ups… Page not found – right? This was expected, even though you created a website, Hugo doesn’t give any default experience – aka your site doesn’t have any page to show.\nNext step, let\u0026rsquo;s install Blowfish using git submodules which will make it easier to manage and upgrade to new versions in the future.\ngit submodule add -b main https://github.com/nunocoracao/blowfish.git themes/blowfish Next, create the following folder structure at the root of your code directory - config/_default/. Now you will need to download these files and place them in _default folder you just created. The final structure should look something like this.\nconfig/_default/ ├─ config.toml ├─ languages.en.toml ├─ markup.toml ├─ menus.en.toml └─ params.toml ` Open the config.toml and uncomment the line theme = \u0026ldquo;blowfish” and you are ready to go. Try the running the site again and check the result at https://localhost:1313\nhugo server You should see something like the image below. Not much yet as we didn’t add any content, but the main skeleton for Blowfish is already in place - just requires configuration.\nNow let’s configure the theme.\nFYI This guide will not cover in detail what each parameter available in Blowfish does – for everything available and how to use it, check Blowfish documentation for every option in every file. menus.en.toml # This file defines your menu structure, for the top banner and the footer. For this guide, let’s create two menu sections: one for Posts and one for Tags.\nPosts - will display the full list of entries Tags - automatically generated based on tags placed on each article To achieve this, make sure the following entries exist in the menus.en.toml file. Once the changes are done, you should see the menus appearing by re-running hugo server.\n[[main]] name = \u0026#34;Posts\u0026#34; pageRef = \u0026#34;posts\u0026#34; weight = 10 [[main]] name = \u0026#34;Tags\u0026#34; pageRef = \u0026#34;tags\u0026#34; weight = 30 languages.en.toml # This file will configure your main details as the author of the website. Change the section below to reflect your details.\n[author] name = \u0026#34;Your name here\u0026#34; image = \u0026#34;profile.jpg\u0026#34; headline = \u0026#34;I\u0026#39;m only human\u0026#34; bio = \u0026#34;A little bit about you\u0026#34; # appears in author card for each article The images for the website should be placed in the assets folder. For this step, please add a profile picture to that folder named profile.jpg or change the configuration above to the filename you chose. If you don’t have a profile image available, you can use the one below for the tutorial.\nassets/profile.jpg The last step is configuring your links – social media, GitHub, etc. The file includes all the supported options, but they are commented. You are welcome to uncomment everything and delete the ones you would rather not use. Replace the right links on the ones you decided to keep. You can also change the order.\nparams.toml # This is the main configuration file for Blowfish. Most of the visual options or customization available can be configured using it, and it covers several areas of the theme. For this tutorial, I decided to use a background layout - check other layouts for Blowfish’s landing page - with the Neon color scheme - you can pick a different color scheme if you want to - check this list or create your own.\nAdd an image.jpg to the assets folder which will be the background for the site. You can also download the examples I am using in this tutorial.\nassets/image.jpg Now let’s jump into the params.toml and start configuring the file. I will focus only on the values that need to be changed, don’t delete the rest of the file without reading the docs. Let’s begin by making sure that we have the right color scheme, that image optimization is on, and configure the default background image.\ncolorScheme = \u0026#34;neon\u0026#34; disableImageOptimization = false defaultBackgroundImage = \u0026#34;image.jpg\u0026#34; # used as default for background images Next, let\u0026rsquo;s configure our homepage. We’re going with the background layout, configuring the homepage image and recent items. Furthermore, we are using the card view for items in the recent category. Finally, let’s configure the header to be fixed.\n[homepage] layout = \u0026#34;background\u0026#34; # valid options: page, profile, hero, card, background, custom homepageImage = \u0026#34;image.jpg\u0026#34; # used in: hero, and card showRecent = true showRecentItems = 6 showMoreLink = true showMoreLinkDest = \u0026#34;/posts\u0026#34; cardView = true cardViewScreenWidth = false layoutBackgroundBlur = true # only used when layout equals background [header] layout = \u0026#34;fixed\u0026#34; Now configure how the article and list pages will look. Here are the configurations for those.\n[article] showHero = true heroStyle = \u0026#34;background\u0026#34; showSummary = true showTableOfContents = true showRelatedContent = true relatedContentLimit = 3 [list] showCards = true groupByYear = false cardView = true If you run hugo server again, you should see something like the image below.\nAdding content to your site # Create a folder to place your posts in /content/posts. This was also the directory configured in your menu to list all the articles. Within that folder, let’s create a new directory and give it the name myfirstpost. Within it create an index.md file – your article and place a featured.jpg or .png for in the same directory as the thumbnail for the article. Use the example below to get started. The first lines in the file are the Front Matter, which tell Hugo what the look and experience of the article will be – different themes support different params for this. Check the docs for more info.\n--- title: \u0026#34;My first post\u0026#34; date: 2023-08-14 draft: false summary: \u0026#34;This is my first post on my site\u0026#34; tags: [\u0026#34;space\u0026#34;] --- ## A sub-title Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi nibh nisl, vulputate eu lacus vitae, maximus molestie libero. Vestibulum laoreet, odio et sollicitudin sollicitudin, quam ligula tempus urna, sed sagittis eros eros ac felis. In tristique tortor vitae lacinia commodo. Mauris venenatis ultrices purus nec fermentum. Nunc sit amet aliquet metus. Morbi nisl felis, gravida ac consequat vitae, blandit eu libero. Curabitur porta est in dui elementum porttitor. Maecenas fermentum, tortor ac feugiat fringilla, orci sem sagittis massa, a congue risus ipsum vel massa. Aliquam sit amet nunc vulputate, facilisis neque in, faucibus nisl. You can create additional articles to see what your site will look like once there is content in it. Your site should look like the images below. The main page shows the recent articles, each article is connected through others automatically via related section, you have tag aggregation, and full-text search.\nShip it # The only thing missing is to ship your site. I will be using Firebase for hosting - it’s a free alternative and provides more advanced features if you are creating something more complex. Go to firebase and create a new project. Once that is done, let’s switch to the CLI as it will make it easier to configure everything.\nLet’s install firebase’s CLI - if not on Mac check install instructions on Firebase.\nbrew install firebase Now log in and init firebase hosting for the project.\nfirebase login firebase init Select hosting and proceed.\nFollow the image below for the options I recommend. Make sure to set up the workflow files for GitHub actions. These will guarantee that your code will be deployed once there is a change to the repo.\nHowever, those files will not work out-of-box, as Hugo requires extra steps for the build to work. Please copy and paste the code blocks below to the respective files within the .github folder, but keep the original projectId in the files generated by firebase.\nfirebase-hosting-merge.yml # # This file was auto-generated by the Firebase CLI # https://github.com/firebase/firebase-tools name: Deploy to Firebase Hosting on merge \u0026#39;on\u0026#39;: push: branches: - main jobs: build_and_deploy: runs-on: ubuntu-latest steps: - name: Hugo setup uses: peaceiris/actions-hugo@v2.6.0 with: hugo-version: 0.115.4 extended: true env: ACTIONS_ALLOW_UNSECURE_COMMANDS: \u0026#39;true\u0026#39; - name: Check out code into the Go module directory uses: actions/checkout@v4 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Build with Hugo env: # For maximum backward compatibility with Hugo modules HUGO_ENVIRONMENT: production HUGO_ENV: production run: hugo -E -F --minify -d public - name: Deploy Production uses: FirebaseExtended/action-hosting-deploy@v0 with: repoToken: \u0026#39;${{ secrets.GITHUB_TOKEN }}\u0026#39; firebaseServiceAccount: \u0026#39;${{ secrets.FIREBASE_SERVICE_ACCOUNT_BLOWFISH_TUTORIAL }}\u0026#39; channelId: live projectId: blowfish-tutorial firebase-hosting-pull-request.yml # # This file was auto-generated by the Firebase CLI # https://github.com/firebase/firebase-tools name: Deploy to Firebase Hosting on PR \u0026#39;on\u0026#39;: pull_request jobs: build_and_preview: if: \u0026#39;${{ github.event.pull_request.head.repo.full_name == github.repository }}\u0026#39; runs-on: ubuntu-latest steps: - name: Hugo setup uses: peaceiris/actions-hugo@v2.6.0 with: hugo-version: 0.115.4 extended: true env: ACTIONS_ALLOW_UNSECURE_COMMANDS: \u0026#39;true\u0026#39; - name: Check out code into the Go module directory uses: actions/checkout@v4 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Build with Hugo env: # For maximum backward compatibility with Hugo modules HUGO_ENVIRONMENT: production HUGO_ENV: production run: hugo -E -F --minify -d public - name: Deploy preview uses: FirebaseExtended/action-hosting-deploy@v0 with: repoToken: \u0026#39;${{ secrets.GITHUB_TOKEN }}\u0026#39; firebaseServiceAccount: \u0026#39;${{ secrets.FIREBASE_SERVICE_ACCOUNT_BLOWFISH_TUTORIAL }}\u0026#39; expires: 30d channelId: preview-${{ github.event.number }} projectId: blowfish-tutorial The last step is committing your code to GitHub and let the workflows you created take care of deploying your site. Since we configured GitHub actions, this will trigger a job that will configure and deploy your site automatically.\ngit add . git commit -m \u0026#34;add github actions workflows\u0026#34; git push If the actions tab for your repo, you should see something like this.\nOnce all the steps finish, your Firebase console should show something like the image below - including the links to see your app – I got a version of this tutorial running on https://blowfish-tutorial.web.app/.\nConclusion and Next Steps # Now you have your first version of your homepage. You can make changes locally and once you commit your code they will automatically be reflected online. What shall you do next? I’ll leave you with some useful links to get you inspired and learn more about Blowfish and Hugo.\nhttps://blowfish.page/docs/ https://blowfish.page/docs/configuration/ https://blowfish.page/docs/shortcodes/ https://blowfish.page/examples/ https://blowfish.page/users/ https://gohugo.io/documentation/ ","date":"2023-10-04","externalUrl":null,"permalink":"/posts/202310-blowfish-tutorial/","section":"Posts","summary":"Just one year ago, I created Blowfish, a Hugo theme crafted to build my unique vision for my personal homepage. I also decided to make it an open-source project. Fast-forward to today, and Blowfish has transformed into a thriving open-source project with over 600 stars on GitHub and a user base of hundreds. In this tutorial, I’ll show you how to get started and have your website running in a couple of minutes.","title":"Build your homepage using Blowfish and Hugo"},{"content":"","date":"2023-10-04","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"","date":"2023-10-04","externalUrl":null,"permalink":"/categories/open-source/","section":"Categories","summary":"","title":"Open-Source"},{"content":"","date":"2023-10-04","externalUrl":null,"permalink":"/tags/tutorial/","section":"Tags","summary":"","title":"Tutorial"},{"content":"Docker Desktop 4.23 is now available and includes numerous enhancements, including ASP.NET Core support in Docker Init, Configuration Integrity Check to alert on any configuration changes that require attention, and cross-domain identity management. This release also improves Quick Search, allowing for searching across containers, apps, Docker Hub, Docs, and any volume, and performing quick actions (start/stop/delete). VirtioFS is now set to default to provide performance gains for Mac users. With the Docker Desktop 4.23 release, Mac users will also see increased network performance using traditional network connections.\nIn this post, we dive into what’s new and updated in the latest release of Docker Desktop.\nASP.NET Core with Docker Init # We are excited to announce added support for ASP.NET Core. Whether you’re new to Docker or a seasoned pro, Docker Init now streamlines Dockerization for your ASP.NET Core projects. With a simple docker init command in your project folder and the latest Docker Desktop version, watch as Docker Init generates tailored Dockerfiles, Compose files, and .dockerignore files.\nSpecific to ASP.NET Core, we also improved support and comments in the DockerFile for multi-arch builds. This advancement will help users manage sharing their images across different CPU architectures and streamline deployments on cloud providers such as Azure, AWS, and GCP. Create images that fit various architectures, boosting flexibility and efficiency in cloud deployments.\nGet started by ensuring you have the latest Docker Desktop version. Then, execute docker init in your project directory through the command line. Let Docker Init handle the heavy lifting, allowing you to concentrate on your core task — crafting outstanding applications!\nConfiguration Integrity Check # Ensure Docker Desktop runs smoothly with our new Configuration Integrity Check. This allows you to keep using multiple local applications and tools, sometimes making configuration changes hassle-free. This update automatically detects and alerts to any configuration changes, prompting a simple click and re-establishing Docker Desktop configurations to track and ensure uninterrupted development.\nCross-domain identity management # User access management for Docker just got more powerful. SCIM helps auto-provision or de-provision users, and group role mapping is now supported so you can organize your teams and their access accordingly:\nYou can assign roles to members in your organization in the IdP. To set up a role, you can use optional user-level attributes for the person you want to assign a role. You can also set an organization and team to override the default provisioning values set by the SSO connection. The following table lists the supported optional user-level attributes.\n## Improvements to Quick Search Empowering developers with seamless access to essential resources whenever they’re needed, our revamped Quick Search feature has received significant upgrades. Now, users can effortlessly locate:\nContainers and Compose apps: Easily pinpoint any container or Compose app residing on your local system. Additionally, gain quick access to environment variables and perform essential actions such as starting, stopping, or deleting them. Docker Hub images: Whether it’s public Docker Hub images, local ones, or those from remote repositories, Quick Search will provide fast and relevant results. Extensions: Discover more about specific extensions and streamline installation with a single click. Volumes: Effortlessly navigate through your volumes and gain insights into the associated containers. Documentation: Instantly access invaluable assistance from Docker’s official documentation, all directly from within Docker Desktop. Enhanced Quick Search is your ultimate tool for resource discovery and management, offering unmatched convenience for developers.\nStandardizing higher performance file sharing with VirtioFS for Mac users # Docker Desktop 4.23 now defaults to utilizing VirtioFS on macOS 12.5+ as the standard to deliver substantial performance gains when sharing files with containers through docker run -v or bind mounts in Compose YAML. VirtioFS minimizes file transfer overhead by allowing containers to access files without volume mounts or network shares.\nSkipping network file transfer protocols also leads to faster file transfers. We measured performance improvements, decreasing file transfer time from 7:13.21s to 1:4.4s — an 85.15% increase in speed. We do want to note that the measured improvement depends on the size of the file, what other apps are running, and the hardware of the computer.\nDocker Desktop network speed improvements for Mac users # Docker Desktop 4.23 comes with improved networking performance for Mac users. Now, when a container requires a traditional network connection, users will experience increased network performance in these ways:\nAccessing exposed ports ~10x faster Transmission Control Protocol (TCP) ~1.5x – 2x faster No user action is required to experience these benefits — all Mac users updated to 4.23 will now network faster!\nConclusion # Upgrade now to explore what’s new in the 4.23 release of Docker Desktop. Do you have feedback? Leave feedback on our public GitHub roadmap, and let us know what else you’d like to see in upcoming releases.\nLearn more Read the Docker Desktop Release Notes. Get the latest release of Docker Desktop. Have questions? The Docker community is here to help. New to Docker? Get started.\nNote: this post was originally posted externally please go to Docker\u0026rsquo;s blog to read the full post ","date":"2023-09-12","externalUrl":"https://www.docker.com/blog/docker-desktop-4-23/","permalink":"/posts/202309-docker-423/","section":"Posts","summary":"Docker Desktop 4.23 is now available and includes numerous enhancements, including ASP.NET Core support in Docker Init, Configuration Integrity Check to alert on any configuration changes that require attention, and cross-domain identity management.","title":"Docker Desktop 4.23"},{"content":"","date":"2023-07-30","externalUrl":null,"permalink":"/tags/books/","section":"Tags","summary":"","title":"Books"},{"content":"Are you a product manager seeking to enhance your understanding of design and user experience? \u0026ldquo;The Design of Everyday Things\u0026rdquo; by Don Norman, is a captivating and enlightening exploration of the design principles that govern the most seemingly simple objects around us.\nOverview # In this book, Don Norman takes readers on a fascinating journey into the world of design, where he dissects the intention and thought processes behind everyday objects that we often take for granted. Norman delves into the fundamental principles of good design and the psychological aspects that influence our interactions with the products we use daily.\nWhy It\u0026rsquo;s a Must-Read for Product Managers # As a product manager, your primary goal is to create products that are not only functional but also intuitive and enjoyable for users. Norman\u0026rsquo;s book serves as an invaluable guide for understanding the critical role of design in shaping user experiences. The book also demystifies the design process, unraveling the intricacies that go into creating well-designed products. Norman explains the significance of affordances, signifiers, and mapping, which are vital concepts for product managers to consider when designing user-friendly experiences.\nUnderstanding users\u0026rsquo; needs and behaviors is essential for product managers. This book emphasizes the importance of empathy and user-centric design, enabling product managers to better align their creations with the preferences and expectations of their target audience. By learning from the pitfalls of poor design through real-world examples, product managers can avoid similar mistakes and create more efficient, engaging, and successful products.\n\u0026ldquo;The Design of Everyday Things\u0026rdquo; also encourages readers to adopt a design mindset, where even the simplest objects become opportunities for innovation and improvement. Product managers can embrace this approach to foster creativity and find novel solutions to challenges in their projects. By creating products that empower users through ease of use and understanding, product managers can build experiences that make users feel confident and in control when interacting with their products.\nConclusion # \u0026ldquo;The Design of Everyday Things\u0026rdquo; is a must-read for product managers looking to expand their horizons and refine their approach to design. By delving into the underlying principles and thought processes behind everyday objects, this book offers valuable lessons for crafting user-centric, intuitive, and successful products. With its engaging narrative and practical takeaways, it opens minds to the profound impact of design choices, transforming the way product managers perceive and create the products of tomorrow.\n","date":"2023-07-30","externalUrl":null,"permalink":"/posts/202307-design-of-everyday/","section":"Posts","summary":"Are you a product manager seeking to enhance your understanding of design and user experience? \u0026ldquo;The Design of Everyday Things\u0026rdquo; by Don Norman, is a captivating and enlightening exploration of the design principles that govern the most seemingly simple objects around us.","title":"Books I've Read - The Design of Everyday Things"},{"content":"","date":"2023-07-30","externalUrl":null,"permalink":"/categories/design/","section":"Categories","summary":"","title":"Design"},{"content":"In the fast-paced and competitive world of entrepreneurship, achieving product-market fit (PMF) is the holy grail. It is the moment when a product or service aligns perfectly with the needs and desires of the target market, leading to enthusiastic customer adoption and sustainable growth. But how do entrepreneurs know if they have truly achieved this elusive state? In this article, we will explore the essence of product-market fit, delve into its significance for entrepreneurs and investors, and discuss the HUNCH framework—a valuable tool for assessing whether a company has achieved PMF.\nWhat is Product-Market Fit? # Product-market fit refers to the ideal state where a product\u0026rsquo;s value proposition aligns perfectly with the needs and demands of the target market. At this stage, customers are not only attracted to the product but also become enthusiastic users and advocates. Achieving product-market fit signifies that a business has a profound understanding of its target audience and has crafted a solution that effectively addresses their pain points or fulfills their desires.\nWhy is it Important? # Product-market fit holds immense importance for entrepreneurs and investors alike. It acts as a catalyst for accelerated growth, giving businesses a competitive edge in the market. By aligning the product with the market\u0026rsquo;s needs, businesses can foster customer loyalty and create a sustainable revenue stream. Investors look for product-market fit as a validation of a startup\u0026rsquo;s potential for success and scalability.\nMoreover, the correlation between product-market fit and a successful future for a venture is undeniable. It sets the stage for accelerated growth and sustainability, allowing businesses to scale confidently. Strong unit economics, a byproduct of successful product-market fit, ensure profitable customer acquisition and attract investor interest, fueling exponential growth and long-term success.\nDo You Have It? # Determining whether a company has Product-Market Fit it not a 100% exact science, but it is definitely way more scientific and measurable than some people would tell you. An interesting framework to check if a venture has achieved PMF is the HUNCH framework:\nHair on fire value proposition Usage high NPS Churn low High LTV/CAC Hair on Fire Value Proposition # Is the value proposition a must-have need for the target customer, vastly superior to alternatives, and likely to generate high demand and customer enthusiasm? Analyze customer feedback and testimonials to gauge their excitement and satisfaction with the product.\nUsage High # Examine customer engagement and usage patterns to ensure the product is becoming an integral part of their routines or workflows, and its usage is growing over time. High usage indicates that the product addresses a genuine need and delivers value.\nNPS Greater Than 40 # Calculate the Net Promoter Score (NPS), which measures customer satisfaction and loyalty. A score greater than 40 indicates strong customer advocacy, suggesting that customers are likely to recommend the product to others.\nChurn Low # Ideally, the churn rate should be less than 3% per month, demonstrating enduring value and customer satisfaction over time. Low churn rates indicate that customers find ongoing value in the product and are less likely to switch to competitors.\nHigh LTV/CAC Ratio # Aim for an LTV/CAC ratio greater than 3, indicating positive unit economics and the ability to acquire customers profitably at scale. A high ratio proposes that the product generates sufficient revenue from customers to cover the cost of customer acquisition. A ratio between 1 and 3 is still good but not strong, anything under 1 is trouble as it means the company is spending more money to get a customer than the value it gets back from them.\nHow Can You Use It? # Understanding and applying the concept of product-market fit can vary depending on the role and perspective. Here\u0026rsquo;s how you can use it effectively:\n…as a Startup Founder # Founders should use the HUNCH framework to continuously assess their product\u0026rsquo;s fit with the market. Regularly gather customer feedback and analyze usage data to identify areas for improvement. Iterate and pivot as needed to achieve better product-market alignment. The framework also provides insights over which metrics should be important for every company, i.e., LTV, CAC, Churn, NPS, etc. which are great indicators for not only PMF but business health.\n…as an Investor # Investors can use the HUNCH framework as a due diligence tool to evaluate potential investments. Startups that demonstrate strong alignment with the HUNCH criteria are more likely to have achieved product-market fit and offer attractive growth prospects. Again, this is not a 100% scientific process and there are other factors to consider when making an investment decision, what’s the strategy for the company, the market in which it operates, etc.\n…as Product Managers # Product managers play a crucial role in optimizing product-market fit. They can use the HUNCH framework to identify specific areas for improvement and prioritize feature development that aligns with customer needs and desires. It is also a valuable tool when accessing joining a company, specially startups, to get a reading on the stage the teams are at. Correlating that information with the stage of the venture (seed, series A, etc.) will help you understand the company you\u0026rsquo;re joining. E.g., joining a company with a series A (which should already have PMF), more than 150 people, and without strong indicators of PMF might not be a good idea.\nConclusion # Product-market fit is a pivotal milestone that sets the stage for a successful business. While it\u0026rsquo;s not a scientifically quantifiable concept, the HUNCH framework provides valuable data points and signals to identify potential product-market fit. Entrepreneurs who focus on understanding their customers, iterating their products, and measuring key metrics have a higher chance of achieving product-market fit and unlocking the doors to sustained growth and success. Remember, product-market fit is not merely a hunch; it\u0026rsquo;s a culmination of insights, metrics, and customer signals that reveal whether a business has found its rightful place in the market.\n","date":"2023-07-29","externalUrl":null,"permalink":"/posts/202307-pmf/","section":"Posts","summary":"In the fast-paced and competitive world of entrepreneurship, achieving product-market fit (PMF) is the holy grail. It is the moment when a product or service aligns perfectly with the needs and desires of the target market, leading to enthusiastic customer adoption and sustainable growth. But how do entrepreneurs know if they have truly achieved this elusive state?","title":"Product-Market Fit: What it is and do you have it"},{"content":"Docker has revolutionized the way developers build, package, and deploy their applications. Docker containers provide a lightweight, portable, and consistent runtime environment that can run on any infrastructure. And now, the Docker team has developed docker init, a new command-line interface (CLI) command introduced as a beta feature that simplifies the process of adding Docker to a project (Figure 1).\nNote: Docker Init should not be confused with the internally -used docker-init executable, which is invoked by Docker when utilizing the –init flag with the docker run command. With one command, all required Docker files are created and added to your project.\nCreate assets automatically # The new docker init command automates the creation of necessary Docker assets, such as Dockerfiles, Compose files, and .dockerignore files, based on the characteristics of the project. By executing the docker init command, developers can quickly containerize their projects. Docker init is a valuable tool for developers who want to experiment with Docker, learn about containerization, or integrate Docker into their existing projects.\nTo use docker init, developers need to upgrade to the version 4.19.0 or later of Docker Desktop and execute the command in the target project folder. Docker init will detect the project definitions, and it will automatically generate the necessary files to run the project in Docker.\nThe current Beta release of docker init supports Go, Node, and Python, and our development team is actively working to extend support for additional languages and frameworks, including Java, Rust, and .NET. If there is a language or stack that you would like to see added or if you have other feedback about docker init, let us know through our Google form.\nIn conclusion, docker init is a valuable tool for developers who want to simplify the process of adding Docker support to their projects. It automates the creation of necessary Docker assets and can help standardize the creation of Docker assets across different projects. By enabling developers to focus on developing their applications and reducing the risk of errors and inconsistencies, Docker init can help accelerate the adoption of Docker and containerization.\nSee Docker Init in action # To see docker init in action, check out the following overview video by Francesco Ciulla, which demonstrates building the required Docker assets to your project.\nNote: this post was originally posted externally please go to Docker\u0026rsquo;s blog to read the full post ","date":"2023-05-11","externalUrl":"https://www.docker.com/blog/docker-init-initialize-dockerfiles-and-compose-files-with-a-single-cli-command/","permalink":"/posts/202305-docker-init/","section":"Posts","summary":"Initialize Dockerfiles and Compose files with a single CLI command","title":"Docker Init"},{"content":" What is Blitzscaling? # Blitzscaling is a term coined by entrepreneur and investor Reid Hoffman, which refers to a strategy for rapidly scaling up a company in a short period. The idea behind blitzscaling is to prioritize growth over profitability, aiming to dominate a market and become a category leader. While blitzscaling can lead to significant success for companies, it also has some significant downsides.\nOne of the reasons why this approach works is that it allows companies to grow rapidly and capture market share before competitors can catch up. By investing heavily in growth, companies can create network effects, build brand recognition, and attract customers before competitors can establish themselves in the market. This can create a virtuous cycle, where growth begets more growth, and the company becomes dominant in its market.\nDownsides # One of the biggest downsides of blitzscaling is that it can lead to burnout for employees and the company itself. By prioritizing growth over profitability, companies can quickly accumulate debt and become over-leveraged, which can lead to financial instability. Additionally, the pressure to grow quickly can result in a toxic company culture, where employees are overworked and unappreciated, leading to high turnover rates and difficulty attracting top talent.\nBlitzscaling has also contributed to the creation of some problems in the tech industry. For example, some companies have prioritized growth at the expense of ethical considerations, leading to negative outcomes for users and society as a whole. For example, Uber\u0026rsquo;s rapid growth was accompanied by reports of unethical business practices, such as aggressive expansion into new markets and mistreatment of drivers. Similarly, Facebook\u0026rsquo;s growth was accompanied by a failure to address issues such as fake news, hate speech, and privacy concerns, which have had negative impacts on society.\nAwesome When It Works # On the other hand, some companies have successfully used blitzscaling to create positive outcomes for both themselves and society. For example, Amazon\u0026rsquo;s rapid growth allowed it to become a dominant player in e-commerce, which has had significant benefits for consumers in terms of convenience and affordability. Similarly, Google\u0026rsquo;s rapid growth allowed it to create innovative products and services that have positively impacted society, such as Google Maps, Gmail, and Google Translate.\nFinal Thoughts # In conclusion, while blitzscaling can be an effective strategy for rapidly scaling up a company, it also has significant downsides that should not be ignored. The pressure to prioritize growth over profitability can lead to burnout, financial instability, and toxic company culture. Additionally, the pursuit of growth can lead to unethical behavior and negative outcomes for users and society.\nUltimately, companies should carefully consider the trade-offs of blitzscaling and ensure that they prioritize sustainable growth that benefits all stakeholders, rather than simply prioritizing growth at all costs. More than that, it should be clear that scaling a company, with or without blitzscaling, should come after there is a proven strategy to scale. Otherwise, not only there are risks in scaling fast, but also in scaling something which was never proven.\nRecommended article about Blitzscaling from Harvard Business Review\n","date":"2023-04-29","externalUrl":null,"permalink":"/posts/202304-blitzscalling/","section":"Posts","summary":"Blitzscaling is a term coined by entrepreneur and investor Reid Hoffman, which refers to a strategy for rapidly scaling up a company in a short period. The idea behind blitzscaling is to prioritize growth over profitability, aiming to dominate a market and become a category leader. While blitzscaling can lead to significant success for companies, it also has some significant downsides.","title":"Blitzscaling"},{"content":"","date":"2023-04-29","externalUrl":null,"permalink":"/tags/scale/","section":"Tags","summary":"","title":"Scale"},{"content":"We’re always looking for ways to enhance your experience with Docker, whether you’re using an integration, extension, or directly in product. Docker Desktop 4.18 focuses on improvements in the command line and in Docker Desktop.\nRead on to learn about new CLI features in Docker Scout, and find out about Docker init, an exciting CLI Beta feature to help you quickly add Docker to any project. We also review new features to help you get up and running with Docker faster: Container File Explorer, adminless macOS install, and a new experimental feature in Docker Compose.\nDocker Scout CLI # In Docker Desktop 4.17, we introduced Docker Scout, a tool that provides visibility into image vulnerabilities and recommendations for quick remediation. We are delighted to announce the release of several new features into the Docker Scout command line, which ships with Docker Desktop 4.18. These updates come after receiving an overwhelming amount of community feedback.\nThe 4.18 release of Docker Scout includes a vulnerability quickview, image recommendations directly on the command line, improved remediation guidance with BuildKit SBOM utilization, and a preview feature comparing images (imagine using diff, but for container images).\nQuickview # Suppose that you have created a new container image and would like to assess its security posture. You can now run docker scout quickview for an instant, high-level security insight into your image. If any issues are found, Docker Scout will guide you on what to do next.\nCommand-line recommendations # If you’ve previously used docker scout cves to understand which CVEs exist in your images, you may have wondered what course of action to take next. With the new docker scout recommendations command, you receive a list of recommendations that directly suggest available updates for the base image.\nThe docker scout recommendations command analyzes the image and displays recommendations to refresh or update the base image, along with a list of benefits, including opportunities to reduce vulnerabilities or how to achieve smaller image sizes.\nBuildKit provenance and SBOM attestations # In January 2023, BuildKit was extended to support building images with attestations. These images can now use the docker scout command line to process this information and determine relevant next steps. We can support this as the docker scout command-line tool knows exactly what base image you built with and can provide more accurate recommendations.\nIf an image was built and pushed with an attached SBOM attestation, docker scout reads the package information from the SBOM attestation instead of creating a new local SBOM.\nTo learn how to build images with attestations using BuildKit, read “Generating SBOMs for Your Image with BuildKit.”\nCompare images # Note: This is an experimental Docker Scout feature and may change and evolve over time.\nRetrospectively documenting the changes made to address a security issue after completing a vulnerability remediation is considered a good practice. Docker Desktop 4.18 introduces an early preview of image comparison.\nThis feature highlights the vulnerability differences between two images and how packages compare. These details include the package version, environment variables in each image, and more. Additionally, the command-line output can be set up in a markdown format. This information can then be used to generate diff previews in pull requests through GitHub Actions.\nWe’d love to know what scenarios you could imagine using this diff feature in. You can do this by opening up Docker Desktop, navigating to the Images tab, and selecting Give feedback.\nRead the documentation to learn more about these features.\nContainer File Explorer # Another feature we’re happy to announce is the GA release of Container File Explorer. When you need to check or quickly replace files within a container, Container File Explorer will help you do this — and much more — straight from Docker Desktop’s UI.\nYou won’t need to remember long CLI commands, fiddle with long path parameters on the docker cp command, or get frustrated that your container has no shell at all to check the files. Container File Explorer provides a simple UI that allows you to:\nCheck a container file system Copy files and folders between host and containers Easily drag and drop files to a container Quickly edit files with syntax highlighting Delete files With Container File Explorer, you can view your containers’ files at any state (stopped/running/paused/…) and for any container type, including slim-containers/slim-images (containers without a shell). Open the dashboard, go to the Containers tab, open the container action menu, and check your files:\nAdminless install on macOS # We’ve adjusted our macOS install flow to make it super easy for developers to install Docker Desktop without granting them admin privileges. Some developers work in environments with elevated security requirements where local admin access may be prohibited on their machines. We wanted to make sure that users in these environments are able to opt out of Docker Desktop functionality that requires admin privileges.\nThe default install flow on macOS will still ask for admin privileges, as we believe this allows us to provide an optimized experience for the vast majority of developer use cases. Upon granting admin privileges, Docker Desktop automatically installs the Docker CLI tools, enabling third-party libraries to seamlessly integrate with Docker (by enabling the default Docker socket) and allowing users to bind to privileged ports between 1 and 1024.\nIf you want to change the settings you configured at install, you can do so easily within the Advanced tab of Docker Desktop’s Settings.\nDocker init (Beta) # Another exciting feature we’re releasing in Beta is docker init. This is a new CLI command that lets you quickly add Docker to your project by automatically creating the required assets: Dockerfiles, Compose files, and .dockerignore. Using this feature, you can easily update existing projects to run using containers or set up new projects even if you’re not familiar with Docker.\nYou can try docker init by updating to the latest version of Docker Desktop (4.18.0) and typing docker init in the command line while inside a target project folder. docker init will create all the required files to run your project in Docker.\nRefer to the docker init documentation to learn more.\nThe Beta version of docker init ships with Go support, and the Docker team is already working on adding more languages and frameworks, including Node.js, Python, Java, Rust, and .Net, plus other features in the coming months. If there is a specific language or framework you would like us to support, let us know. Submit other feedback and suggestions in our public roadmap.\nNote: Please be aware that docker init should not be confused with the internally-used docker-init executable, which is invoked by Docker when utilizing the –init flag with the docker run command. Refer to the docs to learn more.\nDocker Compose File Watch (Experimental) # Docker Compose has a new trick! Docker Compose File Watch is available now as an Experimental feature to automatically keep all your service containers up-to-date while you work.\n(\u0026hellip;)\nOnce configured, the new docker compose alpha watch command will start monitoring for file edits within your project:\nOn a change to ./web/App.jsx, for example, Compose will automatically synchronize it to /src/web/App.jsx inside the container. Meanwhile, if you modify package.json (such as by installing a new npm package), Compose will rebuild the image and replace the existing service with an updated version. Compose File Watch mode is just the start. With nearly 100 commits since the last Docker Compose release, we’ve squashed bugs and made a lot of quality-of-life improvements. (A special shout-out to all our recent first-time contributors!)\nWe’re excited about Docker Compose File Watch and are actively working on the underlying mechanics and configuration format.\nConclusion # That’s a wrap for our Docker Desktop 4.18 update. This release includes many cool, new features, including some that you can help shape! We also updated the Docker Engine to address some CVEs. As always, we love hearing your feedback. Please leave any feedback on our public GitHub roadmap and let us know what else you’d like to see.\nCheck out the release notes for a full breakdown of what’s new in Docker Desktop 4.18.\nNote: this post was originally posted externally please go to Docker\u0026rsquo;s blog to read the full post ","date":"2023-04-05","externalUrl":"https://www.docker.com/blog/docker-desktop-4-18/","permalink":"/posts/202304-docker-418/","section":"Posts","summary":"We’re always looking for ways to enhance your experience with Docker, whether you’re using an integration, extension, or directly in product. Docker Desktop 4.18 focuses on improvements in the command line and in Docker Desktop.","title":"Docker Desktop 4.18"},{"content":"","date":"2023-03-05","externalUrl":null,"permalink":"/tags/career/","section":"Tags","summary":"","title":"Career"},{"content":"","date":"2023-03-05","externalUrl":null,"permalink":"/tags/organization/","section":"Tags","summary":"","title":"Organization"},{"content":"Product Management roles can change dramatically from company to company. Not just in terms of the job description, but also how these roles fit into a larger product org.\nThe tech industry is constantly growing and evolving, with new products being introduced and updated at an ever-increasing pace. To keep up with this pace, companies require skilled people who can effectively manage the product lifecycle, from ideation to launch and beyond. This is where Product Managers come in.\nHowever, the day-to-day job of a PM can change dramatically from company to company. Moreover, there are radically different approaches to structuring a product organization and its roles. These differences might make it hard for new PMs to understand their roles, where do they fit, and what to expect. In this article, I’ll explore different product roles in the tech industry and the specifics of each of them.\nIndividual Contributor Roles # Individual contributors are professionals who contribute to a team or organization, but do not manage others. ICs may be responsible for a certain function within a team or “own” projects. They often collaborate across functions and teams, influencing others without having positional authority. Evolution in this track for a PM is categorized by the ability to manage larger scopes and complexity without losing the ability to execute and bring results to the company.\nAssociate Product Manager (APM) # An Associate Product Manager, or APM, is an entry-level position in product management. APMs typically work under the supervision of more experienced product managers and are responsible for assisting in product development and supporting the product team in various tasks. APMs can be considered as a pipeline for new product managers as they learn about the industry and product development process.\nProduct Manager (PM) # Product Managers, or PMs, are responsible for leading the product development process, from conception to launch. They work closely with cross-functional teams such as engineering, design, marketing, and sales to ensure the product is delivered on time and meets customer needs. PMs are responsible for defining the product vision and strategy, creating a roadmap, and prioritizing features.\nSenior Product Manager (Sr. PM) # Senior Product Managers, or Sr. PMs, have more experience and are responsible for leading more complex and strategic products. They are responsible for understanding the market, customer needs, and competition, and developing product strategies that align with business goals. Sr. PMs often work across more than one single team/product and collaborate with stakeholders at different levels of the organization.\nStaff Product Manager (Staff PM) # Staff Product Managers, or Staff PMs, have extensive experience in product management and are responsible for managing high-impact products. They work closely with executives to develop long-term product strategies and lead cross-functional teams to deliver high-quality products. Staff PMs are responsible for mentoring other product managers and guiding the product development process.\nPrincipal Product Manager (Principal PM) # Principal Product Managers, or Principal PMs, are responsible for leading the product vision and strategy for the company. They work closely with executives to align the product vision with the company\u0026rsquo;s overall strategy and develop long-term product roadmaps. Principal PMs are responsible for mentoring and guiding other product managers and are considered experts in their fields.\nManager Roles # The following roles involve the direct management of other product roles. Not only do these roles focus on crafting and delivering a vision, strategy, and roadmap, but they also are required to create and manage the product org. Specifically, hire and grow teams, mentor PMs, and work with customers, internal stakeholders, and other product IC positions to deliver the expected outcomes.\nGroup Product Manager (GPM) # Group Product Managers, or GPMs, are responsible for managing a group of products or product lines. They oversee multiple PMs and work with cross-functional teams to develop strategies and roadmaps for each product. GPMs are responsible for ensuring the products under their portfolio align with the company\u0026rsquo;s overall strategy and meet customer needs. GPMs also have people management responsibilities, by directly managing PMs and Sr. PMs reporting to them.\nDirector of Product (DoP) # Directors of Product, or DoPs, are responsible for managing a team of group product managers and overseeing the product development process. They work closely with executives to align the product strategy with the company\u0026rsquo;s overall strategy and ensure the products under their portfolio meet customer needs. DoPs are responsible for developing and implementing product management processes and procedures to improve efficiency and effectiveness.\nChief Product Officer (CPO) # Chief Product Officers, or CPOs, are responsible for the overall product vision and strategy of the company. They work closely with executives and stakeholders to ensure the product strategy aligns with the company\u0026rsquo;s goals and objectives. CPOs are responsible for driving innovation and ensuring the company stays ahead of the competition. They oversee multiple product teams and are responsible for ensuring the product development process is efficient and effective.\nConclusion # In conclusion, product roles in the tech industry are diverse and varied. From APMs to CPOs, each role has specific responsibilities and plays a critical role in the product development process. Even if presently there is still no de facto framework for product roles and product organization, things are improving. In my 10 years in Product, there has never been more information available about both how to improve as an IC or how to grow product teams. Right now is a great moment for everyone who wants to become a Product Manager to do so and help shape and define the future of the area.\n","date":"2023-03-05","externalUrl":null,"permalink":"/posts/202303-pm-roles/","section":"Posts","summary":"Product Management roles can change dramatically from company to company. Not just in terms of the job description but also how these roles fit into a larger product org. In this article, I’ll explore different product roles in the tech industry and the specifics of each of them.","title":"Product Roles"},{"content":"","date":"2023-03-05","externalUrl":null,"permalink":"/tags/roles/","section":"Tags","summary":"","title":"Roles"},{"content":"","date":"2023-02-20","externalUrl":null,"permalink":"/tags/bad-money/","section":"Tags","summary":"","title":"Bad Money"},{"content":"","date":"2023-02-20","externalUrl":null,"permalink":"/tags/deliberative/","section":"Tags","summary":"","title":"Deliberative"},{"content":"","date":"2023-02-20","externalUrl":null,"permalink":"/tags/emergent/","section":"Tags","summary":"","title":"Emergent"},{"content":"","date":"2023-02-20","externalUrl":null,"permalink":"/tags/good-money/","section":"Tags","summary":"","title":"Good Money"},{"content":"Most people consider strategy to be an event, some magical moment that changes the course of a company and defines its success for generations. The tale goes, some smart people get together (or occasionally one solo genius), they decide what they are going to do, and then they start implementing that strategy. That\u0026rsquo;s actually not the way the world works.\nStrategy is not an event, but a process. People deciding which strategy a company should follow don’t have all the information, and it’s almost impossible to predict what will work or not work without testing it. More important than knowing the right strategy is understanding how to develop that right strategy and which methods to develop such a strategy to use depending on the context you are in. In this article, we will explore two types of strategies: emergent and deliberative, as well as the concepts of good money and bad money.\nEmergent Strategy # An Emergent strategy is a type of strategy that emerges as a result of experimentation and learning. It is not pre-planned, but rather evolves, and it’s often used in complex and uncertain environments, where traditional planning is less effective. They allow for innovation and creativity, as a rigid plan does not constrain them.\nTo develop an emergent strategy, you need to be open to experimentation and willing to learn from your experiences and should be prepared to adjust your approach based on feedback from customers, colleagues, and stakeholders. You need to be comfortable with ambiguity and uncertainty, as emergent strategies are not guaranteed to succeed. However, by embracing emergent strategy, you can typically find new and innovative ways to achieve your goals.\nDeliberative Strategy # Deliberative strategy, on the other hand, is a more traditional approach to strategy development. It involves careful planning and analysis, with a focus on identifying the most effective course of action. Deliberative strategy is typically used in more stable and predictable environments, where there is greater certainty about the future. Deliberative strategies are more structured and rigid, as they are based on a pre-defined plan.\nTo develop a deliberative strategy, you need to be analytical and data-driven. You should conduct thorough research and analysis to understand the environment and identify opportunities and threats. You should establish clear goals and objectives, and develop a detailed plan for achieving them. Likewise, you should also be prepared to monitor and adjust your strategy as circumstances change.\nGood Money vs. Bad Money # When developing a strategy, no matter if emergent or deliberative, it is important to consider the concept of good money and bad money. In this theory, money has special characteristics that reflect the requirements of the investors.\nLet’s imagine you are starting your own company, you have an idea and a small team willing to join you on that ride. You are thinking about starting small with a small seed investment, testing your idea with small experiments and learning if it actually works. Meanwhile, and investor wants to give you 100x more money and wants you to grow and hire a massive team very fast. Should you take this money? According to the theory, no.\nThe basic idea of good money and bad money is that the type of money a manager accepts carries specific expectations that must be met. These expectations heavily influence the types of markets and channels that a venture can and cannot target. The very process of securing funding forces many potentially disruptive ideas to get shaped instead as sustaining innovations that target large and obvious markets. Thus, the funding received can send great ideas on a march towards failure.\nIn summary:\nAs emergent ideas are being nurtured during nascent years, money must be patient for growth but impatient for profits. When winning strategies become clear and deliberate ideas need to be carried out, then money should be impatient for growth but patient for profit. When to apply # Deciding when to apply a deliberative strategy versus an emergent strategy depends on the nature of the problem or opportunity you are facing. A deliberative strategy is best suited for situations that are more stable and predictable, where there is greater certainty about the future and when there is factual proof that something will work. For example, a company might use a deliberative strategy to develop a long-term business plan to grow a new product line based on positive results from a smaller experiment.\nOn the other hand, an emergent strategy is better suited for situations that are complex and uncertain, where traditional planning is less effective. For example, a startup might use an emergent strategy to experiment with different products and business models, based on feedback from customers and investors.\nIn general, a deliberative strategy is more appropriate for situations where you can anticipate the future, while an emergent strategy is more appropriate for situations where you cannot and need to test multiple scenarios.\nEmergent Deliberate What Unplanned actions from initiatives that bubble up from within the organization. The product of spontaneous innovation and day-to-day prioritization and investment decisions made by middle managers, engineers, salespeople, and financial staff (decisions made by people who aren’t typically in a visionary, futuristic, or strategic state of mind) Conscious and thoughtful organized action. Generated from rigorous analysis of data on market growth, segment size, customer needs, competitors’ strengths and weaknesses, and technology trajectories. Implemented “top-down When When the future is hard to read, it is unclear what the right strategy should be. This is typical during the early phases of a company’s or product’s life, or when the competitive landscape is changing. A winning strategy has become clear because, in those circumstances, effective execution often spells the difference between success and failure. How Ensure that employees are empowered to surface and elevate new ideas. The strategy must make as much sense to all employees as they view the world from their context as it does to top management so that they will all act appropriately and consistently. Conclusion # In conclusion, developing a strategy requires careful consideration of both emergent and deliberative approaches, as well as the concepts of good money and bad money. The best approach will depend highly on the context you are in and what are you trying to achieve.\nIf you are trying to start your company and just have an idea, you should use an emergent strategy so that you can experiment and test ideas until you figure out what the right strategy is for your company. During that phase, it’s a bad idea to accept bad money as there is no proof of what works, and you’re turning the whole endeavor into a bet. Focus on getting good money, money that is patient for growth, but that will push you to find the profits, i.e., the right strategy was already tested, you just need to grow it.\nIf you already tested what works or doesn’t, or know this based on previous work on your company, then you can focus on a deliberative strategy, assessing the data you have and focusing on growth.\n","date":"2023-02-20","externalUrl":null,"permalink":"/posts/202302-how-to-develop-a-strategy/","section":"Posts","summary":"Most people consider strategy to be an event, some magical moment that changes the course of a company and defines its success for generations. The tale goes, some smart people get together (or occasionally one solo genius), they decide what they are going to do, and then they start implementing that strategy. That\u0026rsquo;s actually not the way the world works.","title":"How to Develop an Effective Strategy"},{"content":" Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization\u0026rsquo;s communication structure.\n- Melvin E. Conway1\nIt doesn\u0026rsquo;t matter if you work in a startup, scale-up, or a larger organization, in any case, the success of a product team typically equals growing such a team. First, you need to hire more people, then split the team, now there’s a group of teams to organize, and after a while, the loop eventually begins again. These changes bring challenges and opportunities to organizations. Here are some strategies for organizing product teams, what they optimize, and in which situation to use them.\nWhat to solve for? # When organizing product teams it is important to consider the following four factors: completeness, independence, clarity, and balance. Spoiler alert: I haven’t found any way to optimize all of them. However, there are some clear patterns in which of these factors matter the most, depending on the stage your organization and those teams will be in.\nCompleteness # Making sure that teams and groups own a domain end-to-end. In a complete domain, teams/groups should be able to build a clear value-based vision and roadmap. Domains need to be tight enough (no holes) and wide enough to bring complete value over time instead of delivering features.\nIndependence # Moving fast is one of the most essential aspects of a team’s success. Making sure that each team is independent over its domain will highly contribute to its ability to move fast and create value overall. Independence is achieved when a team can promote its mission and achieve its goals with the development team they are working with, and with minimum dependencies on other teams. Product dependencies are not limited to development teams and technical dependencies. Additional dependencies include the other PMs, other delivery teams like data, UX, design, marketing, and also stakeholders like legal, compliance, and finance.\nClarity # The domain should be clear for the internal team and external stakeholders. This will ensure that a) the team knows what its core function and goals are and b) that it will be easier to align external stakeholders to the same vision. All the team’s artifacts and docs should aim to convey that clarity, e.g., the team’s name.\nBalance # When creating or splitting domains for product teams, or within a product group, it is important to ensure that there is a balanced distribution in terms of relevance and load of the topics. Otherwise, teams can fall into scenarios where a single team is tackling all the most significant problems for the company with only a limited amount of the total available resources. The balance should also ensure that, to some extent, all groups and teams have a certain level of relevance and impact; otherwise, it could be hard to hire and motivate team members.\nStrategies # Here are some options on how to organize teams and how each strategy compares to the four factors above.\nFunctional # aka by Products, Features, Tech Components\ngraph LR O([Product \u0026 Engineering Org]) subgraph Frontend FPM(Product Manager) FEM(Engineering Manager) FPD(Product Designer) FFD(Frontend Developers) end subgraph Backend BPM(Product Manager) BEM(Engineering Manager) BBD(Backend Developers) end subgraph Platform PPM(Product Manager) PEM(Engineering Manager) PBD(Backend Developers) end O --\u003e Frontend O --\u003e Backend O --\u003e Platform Examples of organizing your teams functionally with 3 teams: frontend, backend, and platform Factor Score Completeness ⭐️ high for startups, drops dramatically with scale Independence ⭐️ ⭐️ Clarity ⭐️ ⭐️ ⭐️ Balance ⭐️ ⭐️ This structure splits groups and teams by functional modules like products, features, components, or layers (FE vs. BE). This option is best suited for a company in an early stage, where heavy lifting is required to deliver even the first releases. The vision and roadmap at this point are typically the overall product ones, and you mostly need the different parts to work well together toward the already defined scope. As organizations scale, this becomes a bad option because as teams grow and their split gets increasingly granular. This leads to a dramatic increase in the level of dependencies between teams, and each team/group’s vision and roadmap are constrained, which results in low customer centricity.\nPros Cons - Clear which team should handle specific feedback/bugs - Fewer dependencies than other options for smaller orgs - Easy to bring the right product person to external product meetings, such as a sales call - Causes confusion when features require infrastructure/architectural updates - Constraints vision/strategy/roadmap to the module, feature, or product level (not very customer-centric) - Requires a lot of cross-team coordination when products are tightly integrated, or have lower-level dependencies (e.g. platform) Customer Journey # graph LR O([Product \u0026 Engineering Org]) subgraph Discovery DPM(Product Manager) DEM(Engineering Manager) DPD(Product Designer) DFD(Frontend Developers) DBD(Backend Developers) end subgraph Purchase PPM(Product Manager) PEM(Engineering Manager) PPD(Product Designer) PFD(Frontend Developers) PBD(Backend Developers) end subgraph Authentication APM(Product Manager) AEM(Engineering Manager) APD(Product Designer) AFD(Frontend Developers) ABD(Backend Developers) end O --\u003e Discovery O --\u003e Purchase O --\u003e Authentication Examples of organizing your teams around a customer journey Factor Score Completeness ⭐️ ⭐️ ⭐️ Independence ⭐️ ⭐️ Clarity ⭐️ ⭐️ ⭐️ Balance ⭐️ ⭐️ In this structure, each team/group is responsible for an overall customer journey, or a specific phase in that journey. For example, in a customer purchase flow, a product team can own user acquisition, another onboarding, another discovery, and another the checkout process. This method requires that each phase in the customer journey has enough substance to it. Often, there are important business metrics that closely mirror the success or failure of customers continuing their journey at those junctures, allowing for delegation of accountability. However, optimizing for specific metrics in parts of the overall flow might not help the overall metrics. This org structure requires a lot of design coordination to ensure a cohesive customer experience across the product(s).\nPros Cons - The approach allows efficient product scaling—the growth team drives customers to the product while other teams enhance product trial and engagement experiences. - Clear metrics you can assign to each product manager, such as conversion from free trial to paid or retention - If team members don’t understand their assigned customer stage, it could lead to inadequate product features, and thus a poor product experience. - Requires tight governance to ensure a consistent and great user experience across customer journey stages Problem Definition # Aka Goals, Metrics, Jobs-to-be-Done\ngraph LR O([Product \u0026 Engineering Org]) subgraph Acquisition ACPM(Product Manager) ACEM(Engineering Manager) ACPD(Product Designer) ACFD(Frontend Developers) ACBD(Backend Developers) end subgraph Activation ACTPM(Product Manager) ACTEM(Engineering Manager) ACTPD(Product Designer) ACTFD(Frontend Developers) ACTBD(Backend Developers) end subgraph Engagement EPM(Product Manager) EEM(Engineering Manager) EPD(Product Designer) EFD(Frontend Developers) EBD(Backend Developers) end subgraph Conversion CPM(Product Manager) CEM(Engineering Manager) CPD(Product Designer) CFD(Frontend Developers) CBD(Backend Developers) end O --\u003e Acquisition O --\u003e Activation O --\u003e Engagement O --\u003e Conversion Examples of organizing your teams around a metrics problem definition, in this case a subset of the AARRR metrics Factor Score Completeness ⭐️ ⭐️ ⭐️ Independence ⭐️ ⭐️ Clarity ⭐️ ⭐️ Balance ⭐️ ⭐️ ⭐️ In this metfhod, each team and group is responsible for a problem definition, which can be translated to a goal, metrics, and jobs-to-be-done. Teams can then touch whichever functionality they believe is going to solve that problem. The main benefit of this approach is pushing accountability to individual product managers. It can result in multiple teams wanting (or needing) to work on the same product components at the same time, and thus no one feeling ownership for those things. This is a good choice for companies with well-established product key performance indicators (KPIs) that capture customer and business outcomes. The difference from the previous method is that the overall concerns across different teams are not necessarily part of a single user flow.\nPros Cons - The customer is always at the center of your product thinking - Easy to assign goals to teams and then measure product success - Easy to delegate decision-making and accountability among product managers - Requires a stable set of KPIs that won’t change often - Requires cross-team roadmap coordination as individual teams may need to touch plenty of product areas to hit goals - It takes time to get into customers’ heads (That’s why it’s important not to jump right into product design, but make sure everyone understands how each department views the customer) User Personas # graph LR O([Product \u0026 Engineering Org]) subgraph Buyer BPM(Product Manager) BEM(Engineering Manager) BPD(Product Designer) BFD(Frontend Developers) BBD(Backend Developers) end subgraph Seller SPM(Product Manager) SEM(Engineering Manager) SPD(Product Designer) SFD(Frontend Developers) SBD(Backend Developers) end subgraph Advertiser APM(Product Manager) AEM(Engineering Manager) APD(Product Designer) AFD(Frontend Developers) ABD(Backend Developers) end O --\u003e Buyer O --\u003e Seller O --\u003e Advertiser Examples of organizing your teams around personas, each team focus on the needs of a specific type of user Factor Score Completeness ⭐️ ⭐️ ⭐️ Independence ⭐️ proportional to independence of needs for each persona Clarity ⭐️ ⭐️ ⭐️ Balance ⭐️ depends on relevance of each persona for the business Each team and group are assigned a persona and become responsible for that persona’s needs end-to-end. Usually used in products with multiple personas, where the needs of the various personas are independent and don’t conflict with each other (e.g., marketplace where there are buyers and sellers). This organization focuses teams on the needs of users, but it requires heavy coordination across teams and groups to avoid duplicating efforts, deviating from established design principles, or taking the product in different directions at the same time.\nPros Cons - Very customer-centric, encourages teams to think about customer needs/outcomes - Simplifies user research, each team can target interviews by the type of person they want to talk to and can become experts in that persona over time - Can pull the product in multiple directions at once - If personas have strong connections between them (e.g., two personas that are buyers) it will lead to clashes and low independence across teams and groups Wrap Up # There’s no single solution for all organizational issues across companies, industries, etc. However, the above strategies provide some interesting ways of avoiding big pitfalls.\nAs an example, if you are working on an early-stage company, it might sense to go with a functional split. Teams and scopes will be crystal clear, and it will get you through the first initial stages of product validation faster. In the same way, if your product already has a well-defined user flow (e.g., e-commerce with Acquisition, Activation, Conversion, etc.), it might be an option to focus each team around one of the stages in the customer flow. This will it make it easier to provide clear KPIs and scopes for each team, and will allow you to scale easily. If you have more than one distinct personas (think buyer-seller type), you can optimize those two experiences clearly.\nAll of these strategies allow you to adapt to your context, and evolve your team\u0026rsquo;s structure as that context changes (because it WILL change). There are no clear answers, and the above suggestions are merely examples of how you can leverage some strategies presented here.\nThe only thing that you shouldn\u0026rsquo;t do is try to mix some of these frameworks inside the same structure. This will generate confusion, unclear dependencies, and noise across our organization.\nAt the end, regardless of which option you choose, as you scale, your goal should always be to make sure that your teams don\u0026rsquo;t lose their customer-focus as that will lead to a) unhappy customers and b) failure.\nAny organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization\u0026rsquo;s communication structure.\n- Melvin E. Conway1\nOriginal wording for the Coway\u0026rsquo;s Law, introduced in 1967, from Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-01-08","externalUrl":null,"permalink":"/posts/202301-how-to-structure-product-teams/","section":"Posts","summary":"It doesn\u0026rsquo;t matter if you work in a startup, scale-up, or a larger organization, in any case, the success of a product team usually equals growing such a team. These changes bring challenges and opportunities to organizations. Here are some strategies for organizing product teams, what they optimize, and in which situation to use them.","title":"How to Structure Product Teams"},{"content":"","date":"2023-01-08","externalUrl":null,"permalink":"/categories/management/","section":"Categories","summary":"","title":"Management"},{"content":"","date":"2023-01-08","externalUrl":null,"permalink":"/tags/team/","section":"Tags","summary":"","title":"Team"},{"content":"Technology has disrupted the music industry several times in history, changing the way we consume it, how it’s distributed and even how it’s made. With the introduction of radio, vinyl records and then portable audio players, our relationship with music has become part of everyday life, much more intimate and diverse, creating a multi billion dollar industry that’s constantly reinventing itself to extract the maximum potential from artists and meet our demands as consumers.\nIn the past couple of years there’s been a great debate around the applicability of non-fungible tokens (NFTs), and the music industry is currently the stage for lots of experimentation with popular artists like Snoop Dogg, Steve Aoki and Mike Shinoda making bold moves in the space. In this article we’ll reflect about the good, the bad, and the ugly facts around NFTs in the music industry.\nStatus Quo # For decades, the music industry was controlled by publishers and studios which would act as gatekeepers to what got recorded and distributed. During that period, recording music was an expensive process that required upfront investment. On the other end, the marketing and distribution machine ensured that whatever got recorded was sold. Artists that were not able to get publishing deals had nearly zero chances of entering the market.\nThat all changed with technology, recording is now so cheap that you can do it on almost any laptop, distributing to the major streaming platforms is cheap and accessible, and marketing can be achieved through social networks ( TikTok, for example is one of the most common marketing vehicles). Even with all these shifts, the music industry is still largely controlled by the major publishers because of the huge machine behind, that markets, promotes, and distributes new music, concerts, social accounts, etc.\nSource: International federation of the phonographic industry Streaming has been the dominant distribution format for some years now. In 2021, around 65% of global recorded music revenues, with Spotify alone accounting for almost half a billion users and almost 200 million paying subscribers.\nFor the large majority of content, digital platforms negotiate streaming rights with major record labels. Sony BMG, Warner, and Universal together hold the rights for around 67.5% of the global recorded music market, which shows us how much power and influence these companies exert in the industry. Artists are at the end of the value chain creating content, performing live, and securing the last slice of the pizza. While this is the status quo for most, technology improvements have been consistently enabling artists to explore DIY alternative ways to build a music career and make a decent living from their work. In recent years, NFTs have captured the attention of many, who ventured in the path of independent music making.\nBut taking a step back, what is actually an NFT? Originally proposed and built as an Ethereum smart contract, non-fungible tokens are a specification to represent ownership and trading of unique assets. Technology-wise, the NFT itself is a simple collection of digital documents that can be stored and consumed using normal computers and smartphones. What’s more interesting about them is the fact that they’re built on top of distributed ledgers, that make it very hard/expensive to tamper with, allowing for a trustless and transparent economy where users are able to trade directly with each other, and the whole transaction record stays public for anyone to audit and verify the ownership of any given NFT.\nThe Good # Royalties # There’s a great number of hops in the value chain between artists and their fans. The machine that makes it possible for commercial music to play on our earphones is a complex one, with a lot of different entities profiting in between. Streaming services have been heavily criticized for low payouts, and record labels were always known for monopolizing rights with unfair contracts that force artists to give up ownership of their songs to the label.\nSource: Data Observatory What NFTs and the crypto asset economy are creating is the possibility to bypass some hops, naturally changing the way business is made. While most of what we see is still experimentation, maintaining rights ownership is what seems to be the main discussion topic.\nOn top of selling their content almost directly to fans, NFT transactions make it possible to automate royalty enforcement mechanisms for artists to earn creator shares on the secondary market. NFT platforms like Open Sea, Rarible, Sound.xyz, and Catalog already include royalties as part of their functionality, but the biggest game changer is that these royalties could actually be enforced by the blockchain protocol itself, further protecting the artists from not receiving a fee when their music is re-sold. On top of this, licensing also becomes more efficient as such a public registry can help avoid disputes and overlapping royalty claims.\nAlternative Ways of Monetization # NFTs make it possible to skip the middleman, and enable a different business model where artists can sell content by themselves. There’s not yet a standard recipe for “what” is actually sold with the NFT, which is why artists are experimenting with different combinations. Electronic dance music star 3LAU was among the first to venture in the NFT space and made a small fortune with an NFT sale to mark the third anniversary of his album Ultraviolet. Content included several exclusive items like a custom song, access to never-before-heard music online, custom art based on the music and new versions of Ultraviolet’s 11 original songs.. Another popular NFT sale was carried out by Grimes who launched a collection of digital art accompanied by exclusive music. More conservative approaches are also showing up, and in this case, Kings of Leon were one of the first to add NFTs to the list of already available distribution channels like Spotify and iTunes, along with some special perks for NFT holders.\nAnother interesting thing is again rights, but from the transferability perspective. With no record label contract in place, it is up to the artists to decide whether to protect their contents by keeping the rights to themselves or give them away to NFT holders. There was a popular discussion on twitter between Mike Shinoda and one of his fans, where the artist was very clear about non commercial use when asked if Ziggurat could be reproduced in social media posts. On the other hand, should artists choose to embed copyrights in the NFT, it would open the rights market to the general public looking out for potential investments and portfolio diversification, making it more valuable in the short term.\nScarcity and New Ways to Interact With the Fans # The element of scarcity is known to generate lucrative profits on pretty much everything that can be sold. Paintings and limited product editions are common examples of scarcity augmenting the price of a given asset. The music industry is no stranger to this marketing technique, and the unique nature of NFTs make them the perfect vehicle for artists to appeal to scarcity in new ways, since they are the ones controlling the number of copies in circulation. One example would be Shawn Mendes’ launching his NFT collection, which includes rare digital versions of his signature accessories such as his Fender guitar and gold ring. While such assets can easily be copied and illegally reproduced, the proof of ownership made possible by NFTs also acts as a guarantee of authenticity, and can be used to build experiences like backstage access, digital collectibles and other exclusive assets for the most dedicated fans. This is what the platform aok1verse (Steve Aoki’s take on NFTs and Metaverse) tries to create by granting exclusive perks to the fans like access to merchandise, real life events, early access to unreleased music and even artistic collaboration.\nThe Bad # NFTs and blockchain promise a more fair and equal future in many industries, but at the moment the underlying technology is still pretty immature, and we don’t know in which terms it will stick around. If something changes and all of a sudden Ethereum stops being the dominant NFT platform, users might move away to a different technology and it remains unclear what happens to all of those who already invested in NFTs.\nThere’s a lack of standardization and interoperability which is bringing in new NFT specifications and blockchains. As the ecosystem continues to evolve, we’re likely to see different, incompatible platforms coming to life, potentially leading to an excessive amount of distribution channels for artists to worry about.\nThe usability is also not there yet, and most of these technologies are not accessible to everyone, leaving the less proficient outside, while at the same time creating an opportunity for new middlemen companies that pretty much like producers and distributors will generate profit from the artist’s success.\nOn top of all of this there is the crypto asset market, which is in its early days and is frequently the central stage for price speculation, volatility and fraud. Most of the above problems are mainly related with a general lack of regulation on the market itself which naturally has an impact on how the NFT asset class is perceived.\nThe Ugly # Even with all the above changes, the music industry at its core still has the biggest piece of the pie. Publishers and distributors are still the gatekeepers of success for most artists. Even the ones that found success through social networks still end up getting signed by a major publisher in order to scale their brand and get their music to as many listeners as possible.\nSome of the previously discussed experiments with NFTs are being driven by players in the music industry not by newcomers to the space. Thus, we are yet to understand if NFTs will be just another tool in the publishers’ belt to maximize profits versus something else that can be leveraged at scale by new unsigned artists.\nFinally, most attempts and experiments seem like marketing stunts to leverage the hype of crypto. This has the potential to undermine the future value of NFTs for music both technically and economically.\nWrap Up # The whole crypto asset industry is still at its early days and essentially lacks definition and standardization, which can be both good and bad for its role in the music industry. On one hand it created a space for the brave, defiant ones to experiment new approaches to their careers, but on the other the infrastructure is not built yet, which restricts NFTs to the tech savvy users and creates the space for new middlemen. We should also be mindful that the current price speculation and fraud with NFTs and crypto assets in general doesn’t allow a clear forecast of what will be the monetary value for music NFTs, and neither if it will be possible to make a living out of them.\nWhat stays on the record for now is that by democratizing the commercial trades of digital assets, NFTs have definitely captured some attention in the music industry, helping out artists to have a more independent career and sell their music directly to the fans. Major companies in the industry have also noticed this and will eventually start chipping in as the whole ecosystem is yet to be properly regulated. The latter also poses an interesting question that goes straight to the core of what NFTs and blockchain stand for: if there is a centralization of the system, in this case major studios/publishers controlling most of these technologies, does the community really benefit from decentralized technology?\nEven though NFTs have huge potential for the music industry and art in general, we think that the biggest unfair advantage will probably remain the same as it always has been: the ability to make something stand out of the crowd. And this is where the major studios and publishers win because of their amazingly oiled machine that can pick up almost anything and market/distribute it to a global audience, with or without NFTs.\n","date":"2022-12-22","externalUrl":null,"permalink":"/posts/202212-apes-and-kittens-in-the-music-industry/","section":"Posts","summary":"Technology has disrupted the music industry several times in history, changing the way we consume it, how it’s distributed and even how it’s made. In this article we’ll reflect about the good, the bad, and the ugly facts around NFTs in the music industry.","title":"Apes and Kittens in the Music Industry"},{"content":"","date":"2022-12-22","externalUrl":null,"permalink":"/tags/crypto/","section":"Tags","summary":"","title":"Crypto"},{"content":" Software Engineer ","date":"2022-12-22","externalUrl":null,"permalink":"/authors/davidsimao/","section":"Authors","summary":" Software Engineer ","title":"David Simão"},{"content":"","date":"2022-12-22","externalUrl":null,"permalink":"/tags/music/","section":"Tags","summary":"","title":"Music"},{"content":"","date":"2022-12-22","externalUrl":null,"permalink":"/tags/nft/","section":"Tags","summary":"","title":"Nft"},{"content":"","date":"2022-11-18","externalUrl":null,"permalink":"/categories/interview/","section":"Categories","summary":"","title":"Interview"},{"content":"","date":"2022-11-18","externalUrl":null,"permalink":"/tags/open-source/","section":"Tags","summary":"","title":"Open-Source"},{"content":"","date":"2022-11-18","externalUrl":null,"permalink":"/tags/podcast/","section":"Tags","summary":"","title":"Podcast"},{"content":" Today I got the chance to sit down with David Large from CloudCannon to talk about open-source, Hugo, and my theme, Blowfish. I\u0026rsquo;ve started this open-source project a couple of months ago and the response from the community has been amazing. Check out the link below if you want to learn more about it and how you can contribute.\nBlowfish A powerful, lightweight theme for Hugo built with Tailwind CSS. site ","date":"2022-11-18","externalUrl":null,"permalink":"/posts/202211-static-feedback/","section":"Posts","summary":"Today I got the chance to sit down with David Large from CloudCannon to talk about open-source, Hugo, and my theme, Blowfish.","title":"Static Feedback #4 — Creating the Blowfish Hugo Theme"},{"content":"","date":"2022-11-02","externalUrl":null,"permalink":"/tags/engineering/","section":"Tags","summary":"","title":"Engineering"},{"content":"The product manager role has been gaining popularity in the tech industry over the recent years. As more companies add PMs to their organisation charts, there is still a lot of experimentation with team setups to find the best alignment possible between product and engineering. These two functions work now as close as they ever did, and while it is said to be the recipe for high achieving teams, lots of companies are still struggling to achieve good levels of collaboration.\nStrategies to make it work are well covered in Martin Fowler’s website, in this article I’ll focus on more on the engineer’s perspective and what are our expectations for a good product manager.\nWhat Not to Expect # As an engineer myself I have observed friction coming from both sides, but also some productive partnerships, and while one can argue that the team or the organisation can influence the outcome, it mostly depends on how much each function is willing to collaborate with the other.\nLet’s do an exercise and think about these expectations in reverse. I believe the PM role is still early days, and because of that, it assumes different shapes, especially in less mature companies that are starting to build their product development strategies. If you work in the tech industry right now, you might be familiar with some of the following stereotypes.\nExcel Manager # An ace with macros and a master at reporting progress in the weekly steering. The whole project looks like a geometric piece of art with bars stacked over each other, horizontally aligned with a column of red cells that turn green once you type in the word “Delivered”. The excel manager cares very little about the product lifecycle and will spend all of her chips in getting the devs to commit to those deadlines.\nFeaturist # Top specialist in 360 market research. She knows all about Steve Jobs and the story of the iPod, and care about the product lifecycle, but can’t afford to loose time building strategies because “Details matter, it’s worth waiting to get it right”. As long as build a responsive design, with social share buttons, cash will start flowing. Be sure to check out “ Avoiding Featurism”, which I stole the word featurist from.\nRetired Programmer # Displeased with the idea of being code monkey forever, she abandoned engineering in the search for happiness and success. Looking with regret at the life she left behind, the retired programmer is good ally and is willing to manage the leadership expectations and push back deadlines at the exchange of sharing some ideas for the software architecture and also that story about getting an A+ on a programming project at the university.\nKing’s Hand # Why sharing one’s ideas when we’re all here to serve a greater purpose ? Like an all-pass filter, the king’s hand is taking no chance at fingers pointing on her direction. She’s just the messenger and if you don’t agree, be sure to expect some escalations so that you can all ask guidance from the master.\nThe Single Idea of Product Management # Now [puts serious tone], what the above stereotypes have in common (intentionally) is that all of them delegate business calls to the leadership layer, which I think is the biggest game changer about the product manager role. More than design or implementation, the PM is accountable for the entire product lifecycle from idea through implementation to customer feedback and market performance. A good PM will as as a lower level CEO, holding herself accountable for the the success of a small part of the business (the product), offloading this responsibility from leadership. An even better PM will share this accountability across the entire product engineering team.\nThat being said, how the partnership is implemented is a different story. The most successful teams I’ve integrated are the ones where the PM is there, sometimes even under the same leadership/reporting line. The less friction there is between the two functions, the better results. PM \u0026amp; EM: Rules of engagement establishes 3 foundational rules which I believe can be extended beyond PM and EM to product and engineering: Trust, joint accountability and separate ownership. While it is important that each function plays a different part, working as a team with shared responsibility will increase everyone’s alignment towards what and why they’re doing it, as well as reducing the bureaucracy required to get something done, giving the PM better flexibility to build and adapt the product strategy iteratively.\nOnboard the Team Into the Business # One of the things that always bothered me is how little engineers know about the products they’re building. Surprise or not it is possible to work an entire lifetime without knowing who uses the software you’re building and how much money does it make. One of the advantages of doing business at a lower level, is that this barrier can be broken. Recurring discussions with the team about the product’s performance as well as the north star or other relevant kpis is a powerful way of fostering innovation and keeping motivation levels high.\nOne Roadmap to Rule Them All # Building a technical roadmap while working on a product team was one of the most counter productive experiences I’ve had. While it’s important to keep track of tech debt that needs to be paid, if there’s no buy in from product, experience tells me that those tasks are never going to be implemented.\nSqueezing a couple of tasks into ongoing projects is not sustainable and at some point nor will be the code base leading to developer burnout and degraded product performance. A good PM is able to understand the cost of not paying technical debt and will include it as part of the the product strategy.\nTarget Dates, Not Deadlines # If you want to stress out an engineer, ask him for an ETA or to commit to a deadline set by leadership. Building software under pressure only causes harm to the business in a sense that it will force people into making more mistakes that can cost money or decrease the team’s velocity later on.\nWhile it’s also not acceptable that engineers are free to waste large amounts of time, the PM should be flexible enough to either allow the target date to move or scope to be removed.\nFinal Remarks # What the perfect PM should be like is still an open question, but it is clear that if both product and engineering work towards building an effective partnership, the results can be far more productive as opposed to working in silos (grouped by activity instead of outcome). Building the product from inside the team is key to build a better collaboration environment where high levels of alignment, motivation and productivity are more likely, similar to startup companies. From an engineer’s point of view, the ideal PM is not a stakeholder but a peer instead, pretty much like the CEO of a small startup inside the wider company.\n","date":"2022-11-02","externalUrl":null,"permalink":"/posts/202211-engineering-friendly-pm/","section":"Posts","summary":"The rules of engagement in solid partnership with engineering. The do\u0026rsquo;s and don\u0026rsquo;ts as seen from a software developer\u0026rsquo;s perspective.","title":"Engineering Friendly Product Manager"},{"content":"","date":"2022-11-02","externalUrl":null,"permalink":"/categories/guest/","section":"Categories","summary":"","title":"Guest"},{"content":"","date":"2022-11-02","externalUrl":null,"permalink":"/tags/product/","section":"Tags","summary":"","title":"Product"},{"content":" A friendly (for now) AI ","date":"2022-10-30","externalUrl":null,"permalink":"/authors/artbyaghost/","section":"Authors","summary":" A friendly (for now) AI ","title":"Art by a Ghost"},{"content":"","date":"2022-10-30","externalUrl":null,"permalink":"/categories/generated/","section":"Categories","summary":"","title":"Generated"},{"content":"","date":"2022-10-30","externalUrl":null,"permalink":"/tags/gpt-3/","section":"Tags","summary":"","title":"GPT-3"},{"content":"","date":"2022-10-30","externalUrl":null,"permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"Machine Learning"},{"content":"","date":"2022-10-30","externalUrl":null,"permalink":"/tags/openai/","section":"Tags","summary":"","title":"OpenAI"},{"content":" This article was almost completely generated by AI. The first two paragraphs were written by me, copied from my previous article. This is an example of what the current technology can do. Note: apart from the two initial paragraphs I didn\u0026rsquo;t provide any other additional guidance or input to the model, I also didn\u0026rsquo;t change the output in any way. In the last few years, the hype around artificial intelligence has been increasing (again). Most of it is due to companies like OpenAI, Google, DeepMind (Google subsidiary), Meta, and others producing truly groundbreaking research and innovative showcases in the field. From machines winning complex games like Go and Dota 2to a variety of content generation techniques that produce text, images, audio, and now video, these technologies will have an impact on our future.\nIt feels like we have experienced this hype towards AI in the past, but it never really materialized into anything relevant to our lives. From IBM’s Watson attempts to revolutionize healthcare to the prophecies of self-driving cars, we have been told about how AI will improve our society, yet there always seems to be something preventing us from getting there. On one side, technology might not be there yet for some of those advanced problems, in another, humans tend to be skeptical of machines taking over some of our areas of expertise (Skynet didn’t help here).\nFrom this point onwards the text was generated by a GPT-3 model. In any case, it is important to understand that the current hype around AI is different. Not only are the techniques more advanced, but also the industry and society are much more prepared to accept and use these technologies in our everyday lives. Let’s explore some of the most relevant applications of AI that will have an impact in the near future.\n1. Health Care # One of the most promising applications of AI is in the field of health care. Machines have been used in medicine for a long time, but their capabilities were always limited to simple tasks such as image recognition or data processing. With the advances in deep learning, we are now seeing AI being used for much more complex tasks such as diagnosis, treatment planning, and even surgery.\nOne example of this is IDx-DR, a machine learning system that can detect diabetic retinopathy from retinal images with high accuracy. This is a condition that affects millions of people around the world and can lead to blindness if left untreated. The ability to automatically detect this disease would be a huge step forward in preventing vision loss.\nAnother example is IBM Watson for Oncology, a system that uses machine learning to provide treatment recommendations for cancer patients. The system has access to vast amounts of data on past cases and treatments, which it can use to find patterns and make better decisions. In one study, Watson was able to match or exceed the performance of human experts in recommending treatments for lung cancer patients.\n2. Self-Driving Cars # Self-driving cars have been one of the most hyped applications of AI in recent years. And for good reason: they have the potential to revolutionize transportation and make our roads much safer. Several companies are already testing self-driving cars on public roads, and it is only a matter of time until they become mainstream.\nOne of the challenges with self-driving cars is that they need to be able to handle all sorts of different situations, many of which are rare or unpredictable. This is where machine learning comes in: by training models on large datasets of real-world data, we can teach them how to drive safely in a variety of conditions.\n3. Robotics # Robotics is another area where AI is starting to have a big impact. One example is Boston Dynamics\u0026rsquo; SpotMini robot dog, which can autonomously navigate its environment and perform tasks such as opening doors or picking up objects. Another example is Honda\u0026rsquo;s ASIMO robot, which can walk and run like a human being (albeit slowly). These robots are just scratching the surface of what\u0026rsquo;s possible; as AI technology continues to improve, we will see ever more capable robots being developed for both industrial and consumer applications.\n4. Smart Homes # Smart homes are another area where AI is starting to have an impact. There are already a number of commercial products available that use AI to perform tasks such as controlling the lights or thermostat, playing music, or even making coffee. In the future, we will see even more advanced applications such as automatic laundry and dishwashing, robotic vacuum cleaners, and so on.\n5. Predictive Analytics # Predictive analytics is a branch of AI that deals with making predictions about future events. This can be used for a variety of applications such as financial forecasting, demand planning, or fraud detection. For example, a predictive model could be used to automatically flag suspicious transactions in order to prevent fraud. Or it could be used to predict how much demand there will be for a product in the future, so that companies can plan their production accordingly.\n6. Natural Language Processing # Natural language processing (NLP) is a subfield of AI that deals with understanding and generating human language. This can be used for a variety of applications such as automatic translation, chatbots, and text summarization. For example, Google Translate uses NLP to automatically translate between languages. And chatbots such as Microsoft\u0026rsquo;s Zo use NLP to understand and respond to user input.\n7. Computer Vision # Computer vision is another subfield of AI that deals with understanding images. This can be used for applications such as object recognition, facial recognition, and image search. For example, Facebook uses computer vision to automatically tag people in photos. And Google Images uses it to provide relevant results when you search for an image.\n8. Cybersecurity # Cybersecurity is another area where AI is starting to have an impact. One example is Deep Instinct, which uses deep learning to detect malware. The system is able to learn from data in order to identify new threats that have not been seen before. Another example is LastPass Password Manager, which uses machine learning to automatically generate strong passwords and flag weak ones.\n9. Marketing # Marketing is another area where AI is starting to have an impact. One example is IBM Watson Advertising, which uses machine learning to automatically optimize ad campaigns for maximum performance. Another example is Amazon Personalize, which uses machine learning to provide personalized recommendations to users based on their past behavior.\n10. Finance # Finance is another area where AI is starting to have an impact. One example is Robo-advisors such as Betterment and Wealthfront, which use algorithms to automatically manage your investment portfolio. Another example is Credit Sesame, which uses machine learning to help you get the best credit card offers based on your spending habits.\nConclusion # In conclusion, AI is going to have a big impact on our lives in the near future. There are already a number of commercial products available that use AI to perform tasks such as controlling the lights or thermostat, playing music, or even making coffee. In the future, we will see even more advanced applications such as automatic laundry and dishwashing, robotic vacuum cleaners, and so on.\nHowever, it is important to keep in mind that AI is still in its early stages of development and there are many challenges that need to be addressed before these technologies can be fully realized. On the bright side, there is a lot of research being done in this field and it is only a matter of time until we see even more amazing applications of AI.\nThis article was almost completely generated by AI. Even the respective social posts to promote it were written by the same AI:\nAre you ready for the future of mankind? A future where artificial intelligence (AI) is commonplace and may even surpass human intelligence? This is the premise of my new article. The article, titled \u0026ldquo;The Future of AI - From The Perspective of an AI\u0026rdquo;, explores the potential implications of AI and how it could change the world as we know it. Whether you\u0026rsquo;re a fan of AI or not, this is an article that is definitely worth reading. It\u0026rsquo;s thought-provoking and eye-opening, and it just might change your mind about the role of AI in our future. So, what are you waiting for? Click the link below to read the article now: #futureofai #artificialintelligence #aiforgood\n","date":"2022-10-30","externalUrl":null,"permalink":"/posts/202211-the-new-ai-hype-ai-written/","section":"Posts","summary":"In the last few years, the hype around artificial intelligence has been increasing (again). Most of it is due to truly groundbreaking research and innovative showcases in the field. From machines winning complex games like Go and Dota 2, to various content generation techniques, these technologies will impact our future.","title":"The Future of AI - From The Perspective of an AI"},{"content":"","date":"2022-10-25","externalUrl":null,"permalink":"/tags/dall-e/","section":"Tags","summary":"","title":"Dall-E"},{"content":"","date":"2022-10-25","externalUrl":null,"permalink":"/tags/midjourney/","section":"Tags","summary":"","title":"Midjourney"},{"content":"","date":"2022-10-25","externalUrl":null,"permalink":"/tags/stable-diffusion/","section":"Tags","summary":"","title":"Stable Diffusion"},{"content":"","date":"2022-10-25","externalUrl":null,"permalink":"/categories/technology/","section":"Categories","summary":"","title":"Technology"},{"content":"In the last few years, the hype around artificial intelligence has been increasing (again). Most of it is due to companies like OpenAI, Google, DeepMind (Google subsidiary), Meta, and others producing truly groundbreaking research and innovative showcases in the field. From machines winning complex games like Go and Dota 2to a variety of content generation techniques that produce text, images, audio, and now video, these technologies will have an impact on our future.\nIt feels like we have experienced this hype towards AI in the past, but it never really materialized into anything relevant to our lives. From IBM’s Watson attempts to revolutionize healthcare to the prophecies of self-driving cars, we have been told about how AI will improve our society, yet there always seems to be something preventing us from getting there. On one side, technology might not be there yet for some of those advanced problems, in another, humans tend to be skeptical of machines taking over some of our areas of expertise (Skynet didn’t help here).\nHowever, this time it feels different. Firstly, use cases are way less ambitious than in the past and have concrete practical (and fun) applications; secondly, research in the last 5-10 years had some of the major leaps ever in the machine and deep learning fields. Generative Adversarial Networks (GANs), Diffusion Models, and Transformer Models are good examples of such breakthroughs. Thirdly, this time around the required technology and processing power are here to enable us to run and train these massive networks.\nIt is estimated that OpenAI spent around $10M to $20M to train its GPT-3 text-to-text model. Cost should be higher with models dealing with images. Where Are We and How We Got Here? # So, where are we right now? In the last 5 to 7 years, several specific innovations and practical applications of AI have brought forward the technology (and its respective implications) to public discussion. Before going into what is already possible, let’s go through the more relevant announcements in the last years.\n2015 - Google creates DeepDream - Read More\nGoogle releases a new method using Convolutional Networks that can dream new images based on its training set. The network could generate new images from cats, for example, after learning from tons of real cat images.\n2016 - Google builds AlphaGo that beats Go world champion - Read More\nAlphaGo which was trained using unsupervised learning techniques to make the network compete against itself millions of times to try to beat itself and get better at the game with each iteration. AlphaGo beat the Go champion and was even able to display Go moves that were never seen, showing that it had beyond learning moves from other games into discovering its unique plays.\n2019 - OpenAI Five beats the Dota 2 champions - Read More\nOpenAI Five was training using similar techniques to AlphaGo, this network went through millions of games against itself and got better and better. The challenge with playing a multiplayer online 3D game like Dota 2 was the immense action space possible to the player. OpenAI proved that by using its models and new training techniques, it was possible to solve these problems successfully.\n2020 - OpenAI reveals GPT-3 - Read More\nGenerative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text. The network was trained on more than 400B text tokens from a giant textual training set. The model can then keep writing text given an initial prompt. The impressive part is that more than being grammatically and syntactically correct, the story being told is coherent across sentences. Take a look at the video below if you want some examples of what it can do. For a more detailed explanation of what is happening, you can check this video in which a network comes up with a very believable story about a scientist that discovered unicorns in South America.\n2021/22 - OpenAI announces Dall-E and Dall-E 2 - Read More and Here\nDall-E and Dall-E 2 are networks trained using diffusion models to be able to generate images from textual prompts. You can write a sentence and the AI will come up with an image for it in a short time frame. The model can output different types of styles, and previous images can be used to guide the creation of new ones.\n2022 - Leap Motion releases Midjourney - Read More\nMidjourney is also a text-to-image model. What someone can do with it is almost identical to Dall-E; however, there is a noticeable difference in the outputs it provides because of the different training sets. Not necessarily meaning that one is better than the other, just different.\n2022 - Stable Diffusion released by a collaboration of Stability AI, CompVis LMU, and Runway with support from EleutherAI and LAION - Read More\nSame as Dall-E and Midjourney, Stable Diffusion is another model to generate images from textual prompts. The main difference is that the entities that created this model made it open-source, which means that anyone can play around with it. This generated lots of buzz around it, as the previous models were proprietary at the time.\nAs of right now, it is possible to use most of these technologies either locally or through a service (e.g., OpenAI API) to generate text and images. It is possible to generate entire chapters for a book from small prompts of text, might not be a ready-to-release output, but at least it will help with writer’s block. It is also possible to generate images from text, images from images, and even in and out paint existing images. Furthermore, it is possible to erase part of an image you have, and have one of these models complete it using either another image or a text prompt. Additionally, it is also possible to extend an existing image using the same techniques (example below).\nIs This Magic? # All of these recent advancements are mainly attributed to three big milestones in Deep Learning research: Generative Adversarial Networks (GANs), Diffusion Models, and Transformer Models.\nGAN was a revolutionary framework for training massive networks without exactly having a complete set of data to do so. At a high level, the method defines that two different networks will try to compete against each other in a game where only one can win, learning and getting better at each interaction. Deepfakes, for example, are usually generated using this method. One network tries to generate a fake image of someone, and another one attempts to guess if it’s a fake or a real one. This method was also used to develop AlphaGO and OpenAI Five.\nThe problem with these techniques is that training is hard, and after the network knows how to fool the second one, there is little to no incentive to try interesting new things.\nEnter Diffusion Models. These models were made so that the issue of generating a valid image doesn’t happen in one step, but along a denoize process that can take N steps. A training set is built by adding different levels of noise to valid real images (and their respective textual descriptions). The learning process then consists of the network learning how to remove noise in small amounts to get to the final image. This increases the control over the learning process and ends up producing networks that can produce a way bigger number of outputs than previously. If you want to learn more about how it all works, I recommend the video below.\nFinally, we have Transformer Models, this was one of the most important advancements in the machine learning field, and arguably one of the cornerstones that makes everything we are seeing today possible. These models are neural networks that can learn context, and therefore infer meaning from sequential data.\nBefore transformers, networks relied on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to learn from a large labeled dataset. These took a long time and money to produce and increased the complexity of the final model. Transformers don’t require labeled datasets because they can find the patterns mathematically. This means that now it’s possible to train the new models with the trillions of images and petabytes of text data available on the internet and in company databases.\nAI Democratization # One of the main differences between this AI hype wave to past ones is that the number of people that can try it and interact with it is way bigger than it ever was. The internet made it possible to create services to explore what is possible and let people play around with it. In some cases, even create new business models for the companies behind these innovations. I am personally still wondering how many people pay OpenAI to play around with Dall-E.\nFrom a different angle, there were never so many of these advances made available as open-source technologies that people can download, play around with, and even build upon of. OpenAI has recently released whisper and its Dall-E 2 model to the public. The Stable Diffusion model is also available to the community and there are already several remarkable projects behind it. If you are interested in running Stable Diffusion locally I wrote a tutorial on it, give it a try if you are interested.\nHow to Run Stable Diffusion On Your Laptop 8 mins\u0026middot; loading Nuno Coração Technology Tutorial AI Stable Diffusion Neural Network One of the companies that have been spearheading these efforts is HuggingFace. The company provides tools that enable users to build, train, and deploy machine learning models based on open-source technologies and code. It also helps numerous parties share their models and build upon each other. An example of this is BLOOM, an open-source large language model created collaboratively among millions of researchers.\nThis AI democratization is a unique characteristic of this new hype wave the world is experiencing, which has the potential to entirely change the outcome of how it will impact our lives because of three reasons:\nUse-cases are fun and everyone can try them - Unlike the self-driving prophecies or the all-knowing healthcare AIs of the 80s, these use-cases are way simpler and ubiquitous, therefore appealing to more people. Almost everyone can try it even if you don’t understand how it works - available through open-source software licenses or via a website almost everyone who wants to, can try these out and have fun with them. The community can build on it easily - The fact that some of these will be open to the public will exponentially increase the innovation that will happen in the space. Ultimately, all the above reasons will contribute to making AI as a whole a more widespread and well-accepted technology, which hopefully will get us away from the pop-culture visions of movies like Terminator or The Matrix.\nWhat Can You Do With It Today? # These models and technologies are commoditizing the ability to generate content, which was the last step in the Idea Propagation Value Chain that had yet to be fundamentally disrupted by technology. The internet already entirely changed how we distribute content (the last part of the chain). Almost every file is digital, can be copied at zero cost, and sent almost instantaneously to anyone on earth. These new technologies will revolutionize the initial stages of the propagation value chain: the creation and substantiation of an idea.\nJust considering the technologies I had a chance to play around with (Dall-e, Midjourney, and Stable Diffusion), the pre-requirements of learning to draw, paint, or model and render 3D content completely go away. Anyone will be able to tell to an artificial agent what they want to see, and it will create it for them.\nAs an example, I\u0026rsquo;ve used Stable Diffusion to generate the thumbnail for this article. I knew more or less what I wanted, so it just became a matter of going through a couple of dozens of ideas until I found something that I liked, some examples are below.\nMoreover, if you run out of ideas, and need help with designing the prompts, there are already entire sites focused on indexing and providing the best prompts with examples of what others created. Lexica and Prompthero are two examples that I’ve tried with great results.\nHowever, images are just the beginning…\nBeyond Images # I started playing around with Stable Diffusion a couple of weeks ago, and I have to admit that the news that came out since then blew my mind. As I was amazed at how easy it currently is to ask an AI to generate images, I realized that some projects are trying to go well beyond that.\nIt began when I came across this re-tweet from MKBHD:\nOh no... https://t.co/ZNivykXQP4\n\u0026mdash; Marques Brownlee (@MKBHD) October 19, 2022 I was surprised that there were already such good results for text-to-video models and that so many companies were working on it. That week, I discovered a startup called Runway which is working on a video editor powered by all of these machine-learning innovations. A couple of days after, I’ve seen articles for Google’s new text-video network, Imagen Video, and Meta’s announcement of Make-a-Video.\nAfter quickly discovering all the work happening also to generate 3D models from text and flat images, and animate 3D modes based on textual descriptions.\nHowever, the most surprising one (and also a bit off-putting due to potential implications) was a podcast I came across of Joe Rogan interviewing Steve Jobs, created by podcast.ai. For those of you who don’t know, Joe Rogan has a widely successfully podcast show that runs for years and Steve Jobs is, well, dead. Those two men never had the chance to be in the same room together, however, and without their permission I imagine, there is 20 min of audio of them talking as if the conversation had happened.\nWhile thinking about the impacts of using these technologies to emulate people who are no longer among us, I came across this article. So, not only there are some examples of people doing this with celebrities, there are companies like DeepBrain AI which already monetize such a service and can create a digital avatar of your lost loved ones.\nPotential Pitfalls # Throughout our history as a species, there were always problems and issues that had to be sorted out after a new invention came along.\nLegal \u0026amp; Ethical # One of the potential pitfalls is the legal and ethical Implications of these new AI systems and their impacts on society. For example, when generating an image using one of the text-to-image models in this article, who owns the final product? The person coming up with the prompt? The team that builds the model? The team that builds the training set? The artists whose images were on that set? All of them? None of them? None of that is sorted out at this stage, and it is already a big concern.\nOne of the relevant discussions happening right now regarding this topic is regarding GitHub’s Copilot product copyright issues. Copilot is an AI that was trained using all code repositories available on GitHub to empower a developer to code faster by turning comments into code, for example. How would you feel, having your code being used to generate potentially millions for a private company without getting a dime for it? There’s more information here if you are interested.\nArtists are also finding out how their art was used to train these models and are not happy about it. Companies and startups also need to worry about IP infringement if they are using any of these solutions, or creating them.\nFinally, there is an even bigger problem when considering that this technology can be used by ill-intentioned people. Generating images from people doing stuff they never did, or saying something they never did. This is the same issue with Deepfakes which already has several research initiatives happening, but it’s still a real concern. For what it is worth, some tools in this article made a great job making sure you can’t generate that type of content by adding safety filters to their services. However, for all the open-source ones, anyone has the power to override those safety measures.\nAll of these are very valid legal concerns within the industry that should be addressed ASAP, or there is a risk that all of it can turn into a legal storm that will take us back years. More than just the legal aspects, this technology has a very real potential to destroy someone’s life; therefore it should be thought through with time and low tolerance for mistakes.\nPerceived Value \u0026amp; Backlash # Initially, I thought that this tech would make everyone a good artist, but after playing around with it, I am not convinced that is the case anymore. What makes a good artist is more than just their raw execution ability. Factors like creativity, what actually do you want to create, and artistic knowledge are of super importance for having a good final product. At this stage, I think that these technologies will enable normal people to be able to create something, but will give current professional artists super-powers that will enable them to take their work to another level.\nHaving said this, the fact that these models enable us as a society to produce more, faster, and at a lower cost, will have an impact on the perceived value of its outputs. As an example, imagine a design department at a given news media publication with around 20 people. If the current technology becomes mainstream, probably that same department will not need 20 people.\nThere was a story not so long, about a journalist from The Atlantic that used Midjourney to generate images for an article and received massive backlash on Twitter. You can read his thoughts on what happened here. Given the already difficult and competitive environment in which some of these artists work, the potential pushback against these tools is understandable. There is a potential real impact on the job market. Even though it will be bad for some people in the short term, the real question is whether it will be good or bad in the long run. This phenomenon is quite common in big technological innovations and has happened several times throughout history.\nNote: There is already a new area called Prompt Engineering, and others might appear soon.\nInterestingly enough, legal concerns and human backlash have always been the major pitfalls for the adoption of any AI system in the past, more so than with technology in general.\nWhat’s Next? # I think the current applications of the already existing technology will be massive, and therefore whatever prediction one can make will have a high degree of uncertainty. These technologies affect the current Idea Propagation Value Chain, specifically in the parts of that chain that was not ever touched until now, creation and substantiation. This fact alone has the potential to impact us more than the internet, which changed the duplication and distribution parts of the chain, ever did. Only those impacts could be a discussion for pages and pages of an entire book series. If you are interested in this part of the topic, I highly recommend Ben Thompson’s article on it.\nWith the disclaimer above, here’s what I think will happen in this space in the next 2 to 5 years.\nLegal issues around ownership will increase until a good solution comes up - We already discussed some potential legal issues in this article, if those are not solved, there is a risk of de-railing everything going on in the space. For the copyright ones, I think the grounds for legal action are muddy, to say the least, which might drag these discussions for years before there is actually any real impact on innovation. Dramatical increase in funding for companies working on these problems – Hype usually means FOMO, which means more money for whoever wants to solve problems in the space (yes, even in the current macroeconomic situation). We are already seeing the early signals around this, with some companies raising some of the biggest seed rounds in history: StabilityAI, the creators of Stable Diffusion, raised a seed of $101M on a post-money valuation of $1B. Jasper, the creator of a content platform for marketers, raised $125M on a $1.5B valuation The tech will start being productized as features in existing products - Some of this tech has the potential to go into image and video editing software today. Companies like Runway are already creating brand-new products with this tech at their core. Incumbent companies like Adobe already started to include these tools in their software, i.e., Dall-E straight into Adobe Creative Cloud. All of these areas will start to merge with cohesive results - I expect to see something happening around this in the next 12 to 18 months. At least some kind of PoC that will merge a minimum of 2 of these areas into something new, i.e., video + audio, or 3D + animation, etc. Games, VR, and the Metaverse - I feel like the biggest potential for this technology is how much it can accelerate content creation (once quality is constant, which is still not the case). Games and 3D content are where I see the biggest problem that these models could solve. Think about the amount of time, resources, and money spent to create characters for a game, including conceptualizing, modeling, rigging, animating, etc. AI tools could make the creation of these huge game worlds more effective and efficient. While we wait to know what will happen across this exciting space, I will keep researching and playing around with these technologies as much as I can. What will you create with these systems? What do you think the impacts of deploying them at scale are? Reach out to me and let me know.\nNote: Meanwhile, I\u0026rsquo;ve created an Instagram account to share my Stable Diffusion creations with the web 😬 \\ View this post on Instagram A post shared by Art by a Ghost (@artbyaghost)\n","date":"2022-10-25","externalUrl":null,"permalink":"/posts/202210-the-new-ai-hype/","section":"Posts","summary":"In the last few years, the hype around artificial intelligence has been increasing (again). Most of it is due to truly groundbreaking research and innovative showcases in the field. From machines winning complex games like Go and Dota 2, to various content generation techniques, these technologies will impact our future.","title":"The New Artificial Intelligence Hype"},{"content":"In the last year, several machine learning models have become available to the public to generate images from textual descriptions. This has been an interesting development in the AI space. However, most of these models have remained closed source for valid ethic reasons. Because of this, even though you can interact with them via some interface, you are limited in the number of things you can test. Until now…\nThe latest of these models is Stable Diffusion, which is an open machine learning model developed by Stability AI to generate digital images from natural language descriptions. This model has become quite popular, mainly because it was the first one to be open-sourced.\nI’ve already played around with Dall-E and Midjourney, but wanted to try to run a model locally and get more freedom to play around with things. I was able to successfully install and run the model on my M1 Pro and on my Windows desktop. This guide details the steps I’ve followed to get everything working on my Mac.\nInitial Notes # A couple of notes before we get to it. I tried several guides online, and I was unable to get a smooth experience with any of them. I had to try numerous repos, solutions, etc. The main goal of this guide is to provide instructions for how to run Stable Diffusion on an M1, which is the one where I found more challenges. Installing on Windows was way more straightforward.\nHaving said this, the repo I ended up going with has detailed guides for all platforms: Windows, Mac, and Linux. Please don\u0026rsquo;t hesitate to use any of those if you are using another platform or if this guide doesn’t work for you on a Mac.\nNote: I didn’t try the above Mac guide, as when I found this repo, I had already figured out most of the workarounds needed to get the model to work. Get the Code # Let’s start with getting the code. I am using InvokeAI\u0026rsquo;s fork of Stable Diffusion, which I forked here. You are welcome to use the original repo from InvokeAI if you\u0026rsquo;d like to. I am going to use my fork to ensure that the guide remains updated and working across time. I ended up choosing this repo because 1) it was the first that I was able to get, and 2) it was part of a few forks that had a Web UI, which make interactions with the whole thing way easier.\nTo begin with, clone the repo into your local machine.\ngit clone https://github.com/nunocoracao/InvokeAI Get the Model # Now, you need to get the actual model that contains the weights for the network. This is the result of massive cycles of training with humongous data sets that a normal user with average hardware can’t even try to compete with. The model is not distributed with the code because of it’s size (around 7.5 GB) and to ensure that users need to comply with a license for it, that’s where Hugging Face comes in.\nJust go to the Hugging Face’s site and login, or create an account if you don’t have one. Once you are set up, click here, accept the terms on the model card, and download the file called sd-v1-4-full-ema.ckpt. After you download the model, go into the code folder and place it within models/ldm/stable-diffusion-v1/ with the name model.ckpt. The folder stable-diffusion-v1 does not exist and needs to be created.\nNote: there are other variants of the model that you can explore, this is the one recommended by most of the repos I’ve seen. Setup Environment # With the code and the model ready, the next step is to set up the local environment to run everything.\nInstall Xcode # The first step is to install Xcode, which is a suite of tools that developers use to build apps for Apple platforms. Xcode can be installed from the App Store, or you can download it from Apple’s Developer site.\nAs defined in the documentation, the Command Line Tools Package is a small self-contained package available for download separately from Xcode and that allows you to do command line development in macOS.\nFor a fresh install, or to make sure you have everything we need, this command should be enough:\nxcode-select --install Install Conda # Most of the solutions I’ve seen use Conda to manage the required packages and environments needed to run the solution. Conda’s guide to install on any platform is super clear, so I advise you to just follow the instructions here. They have two flavours of their software: Anaconda and Miniconda. I tried Miniconda with no success. I ended up using Anaconda which solved some of the problems I was experiencing. Furthermore, I followed this guide, when you are set you can use this command to verify your installation (Note: don’t forget to restart your terminal application):\nconda If the installation process was successful, you should see something like the image below.\nNote: conda will require that both python and pip commands are available in the terminal when creating the environment in the next steps. Please make sure to have them properly configured as in Mac the defaults are python3 and pip3, so most likely you would need to create an alias. Install Rust # When following some other guides, I would always get problems on the next part of the process, building the environments. After many tries, I figured that I was missing the Rust compiler from my system. I followed Rust installation guide here, which amounts to running the following command:\ncurl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh Note: I didn’t go back to all the guides in the repo I am using to check whether this is required or not in their method. Please don\u0026rsquo;t hesitate to try the next step without installing Rust and come back if you run into issues. Build and Turn On the Environment # We’re almost there. Now we will create the ldm environment and activate it before start generating images. To accomplish this, cd into the root of the repo you cloned at the beginning of this guide and create the environment using the following command:\nPIP_EXISTS_ACTION=w CONDA_SUBDIR=osx-arm64 conda env create -f environment-mac.yml If you run into any problems in this step, and you need to rebuild the environment, you have two options: 1) using the command below:\nPIP_EXISTS_ACTION=w CONDA_SUBDIR=osx-arm64 conda env update -f environment-mac.yml If you are on a intel Mac the command should be:\nPIP_EXISTS_ACTION=w CONDA_SUBDIR=osx-64 conda env create -f environment-mac.yml Or 2) go into Anaconda’s folder, delete the environment and create the environment with the original command in this section. After trying several repos, I had to rely on 2) to clean the clutter.\nNow it’s time to activate the environment using:\nconda activate invokeai The last step is to preload the models using the command:\npython scripts/preload_models.py Have Fun… # Now it’s time to start to play around with Stable Diffusion. Run:\npython scripts/invoke.py --full_precision --web And open your browser on localhost:9090\nYou should see a Web interface like the one below.\nChose your first prompt and try out the model, all the images will be saved in output/img-samples. Explore the various models and configurations possible. I’ve been running mine with 512x512 images, around 100 cycles for the final images (5 for the initial variants), and config scale at. 7.5. As a sampler I prefer the results using DDIM, you can find some details on the differences between samplers and some examples in this Reddit thread.\nSince I originally wrote this article there is a new version of InvokeAI\u0026rsquo;s stable diffusion implementation, besides what I\u0026rsquo;ve described above there are lots of new features that you can explore, more details here.\nSome Examples # Here are some examples of images from my initial runs of the model. I have to say that there is some art to making these work. I am leaving here a couple of links to get some ideas on how to start designing prompts.\n10 Best Text to Image Prompts for AI Art Generator Prompthero Disclaimer \u0026amp; Other Options # A couple of things before wrapping up. Even though I installed this on both a Mac and a Windows, and the Mac experience with a M1 Pro was not bad at all, the performance on my Windows machine with an Nvidia RTX 2070 was way better. Most of the images you see on this guide were generated from that system, as it allowed me to be faster in trying variants and getting more quality out of samples I liked.\nOne of the initial goals I had was to be able to extend the model with additional training, aka getting my face into the model and play around with it. Unfortunately, that was not possible as I wasn’t able to run the training methods on the Mac, and my GPU on Windows doesn’t have the right requirements to train the model. Currently, it seems you need at least 16Gb of VRAM to do so.\nUltimately, there are a ton of options for running the Stable Diffusion model, some locally, some in the cloud (e.g., Google Colab), so don’t get frustrated if you want to try this out but don’t have access to a machine that can run it. There are probably other solutions right there that you can use.\nTag me on your creations on social if you get this to work:\n\u0026nbsp; Follow me on Twitter \u0026nbsp; Follow me on Instagram ","date":"2022-10-06","externalUrl":null,"permalink":"/posts/202210-stable-diffusion-tutorial/","section":"Posts","summary":"In the last year, several machine learning models have become available to the public to generate images from textual descriptions. This has been an interesting development in the AI space. However, just recently did this technology became available for everyone to try.","title":"How to Run Stable Diffusion On Your Laptop"},{"content":"","date":"2022-10-06","externalUrl":null,"permalink":"/tags/neural-network/","section":"Tags","summary":"","title":"Neural Network"},{"content":"","date":"2022-10-06","externalUrl":null,"permalink":"/categories/tutorial/","section":"Categories","summary":"","title":"Tutorial"},{"content":"","date":"2022-09-21","externalUrl":null,"permalink":"/tags/apple/","section":"Tags","summary":"","title":"Apple"},{"content":"On the 7th of September, Apple held its “Far Out” iPhone event. New iPhones, Apple Watches, and AirPods were announced, each coming with a plethora of features. Yes, all of it was innovation, but none of it brought the kind of excitement Apple used to. Why is that? And why probably this is the best strategy for Apple.\nApple’s Announcements # Let’s start at the beginning, what happened on the event? Apple launched the new iPhone 14 line, 2 new Apple Watches, and new AirPods Pro. These are all new iterations over existing products, and the company didn’t release any new product line. I will focus just on the iPhones and watches.\niPhone # During the event, Apple launched four new iPhones: iPhone 14, iPhone 14 Plus, iPhone 14 Pro, and iPhone 14 Pro Max. The base models run last year’s, iPhone 13 Pro, A15 Bionic chip.\nBoth Pro models will run on the new A16 Bionic chip. Apple changed the front-facing camera notch into a “dynamic island”, which is an interesting software solution to the front-camera holes. The Pro models will have always-on displays.\nAll models will drop the physical SIM tray in the US and will be eSIM only, and they will have emergency satellite connectivity. If you are in a spot with no Wi-Fi/cell connectivity and require assistance, these new iPhones will be able to connect to satellites to send compressed emergency alerts and messages. Apple says the satellite service will be included for free for two years, but hasn’t yet mentioned the cost after that. I’m really looking forward to the YouTube videos of people testing this last one.\nWatch # Apple Watch Series 8 looks exactly like the Series 7 with a couple of new tricks under the hood. New temperature sensors that can be used to help wearers more accurately track ovulation cycles. A new sensor suite that can detect if you’re in a car crash and begin the process of contacting emergency services. Basically, last year’s model with a couple of new feature you probably don’t need. I was honestly confused when someone announced “low-power mode” as a new feature of the Apple Watch 8. If I remember correctly, I have that since my Apple Watch Series 2.\nApple also launched a new premium watch, the Apple Watch Ultra, designed for athletes. This one brings in plenty of features that an average person will probably never need: the ability to dive to 39 m, a 86db siren if you need to ask for help, improved GPS sensors to help track direction and distance and so on. The really amazing feature, the battery lasts 36 hours.\nWhy It’s Not the Same # People still remember the legendary announcements of the iPod, the iPhone, or the original MacBook Air. However, present day Apple is not the same company that generated those massive innovations. At the time, Apple was on a position of whether to disrupt the market or die. Now, Apple is one of the biggest companies of all time, valued at $2.4T (yes, T as in trillion). As an incumbent, its incentives changed dramatically and for a while now, Apple is in a cycle of Sustaining Innovation.\nSustaining Innovation # In the theory of Disruptive Innovation there are three types of innovations: Sustaining Innovations, Low-End disruptions, and New-Market disruptions. Sustaining innovation, usually happens when a company becomes a market leader and is leading technology innovation in its space. Organizations that reach this stage see their incentives change dramatically.\nInstead of focusing on something different and disruptive, companies end up slowing down their innovation speed and settle for providing ever-better products that can sell for ever-better profits to their very best customers. I.e., the game becomes about staying on the top, not getting there.\nUltimately, sustaining innovation makes good products into better products, which is not a bad strategy at all. Sustaining innovation can be characterized in three different aspects: performance, customers, and business model.\nPerformance # This first aspect means that these types of innovations aim to increase performance in attributes most valued by the company’s most demanding customers (typically, the high-end customers are the ones willing to pay more for premium features on products).\nThese improvements may be either a) incremental or b) breakthroughs to the product. Apple’s event had a couple of examples of both. The iPhones got slightly better with features that probably the average consumer will not need/use (e.g., do you really need a better camera to share on WhatsApp or post images on Instagram?). The Apple Watch Ultra was a good example of a breakthrough, in which it’s a high-end offering for premium customers of smartwatches with tons of new features.\nCustomers # The second characteristic is that usually, the target is the high-end of the market. Why? Because these are the customers who are probably willing to pay more for an even better product. These are also the users who value those premium features that most people don’t care about or are not willing to pay for.\nIf you take a look at the pricing strategy for the new Apple products (specially the premium ones) it’s clear that the target is the high-end as that was where the price spikes happened. The cheapest iPhone Pro is $300 more expensive than last year, and the Apple Watch Ultra starts at $799 which is twice the price of the new series 8.\nBusiness Model # The last element of the puzzle is the Business Model. Sustaining innovations are frequently characterized by an increase (or at least retention) of the profit margins. The incentive comes from the previous points in the sense of shifting from big leaps and bets to a more conservative approach.\nIf you are shipping something slightly better for your high-end customers, it makes sense that you are also going to increase your profit margins. By doing this, companies can show an expectable growth rate every cycle, which is a great message for investors and stockholders. Usually, the better margin is achieved by exploiting the existing processes and cost structures and by making better use of current competitive advantages.\nIf we compare the prices between this year’s event and last year’s one, there is an increase in some products. Given the similarity of the hardware between last year and this year’s models, I would guess the cost remained relatively similar, which would lead to an increase in the profit margin.\nProducts and Starting Prices 2021 2022 Delta iPhone line (13) $799 (13 mini) $699 (14 Plus) $899 (14) $799 $100 per unit iPhone Pro line (13 Pro Max) $1099 (13 Pro) $999 (14 Pro Max) $1099 (14 Pro) $999 n/a Apple Watch (Watch 7) $399 $399 n/a Apple Watch Ultra n/a $799 New revenue What Will Happen Next? # Does Apple feel like a different (and less exciting company) than what it used to? Yes. Is this the right strategy for them? Most likely also a yes. Apple, and similar companies, have achieved such a tremendous net-worth that the incentives to be disruptive simply stop being there, at least in some areas of their business. By turning their hardware/software business into sustaining innovation, they can keep growing at a steady pace every year and be successful in that space. Meanwhile, they are betting on other areas like services, streaming, and financial services.\nEven though as a consumer, I miss the times when Apple launched truly disruptive products, I think this is the best strategy for them at this stage. At the end of the day, either Apple will need to disrupt itself at some point to keep growing, or they will be disrupted by a competitor before that happens. It will definitely be interesting to see how this will pan out in the next 5 to 10 years.\nNote: Sustaining Innovation is one type of innovation in the theory of Disruptive Innovation, a concept developed by the American academic Clayton Christensen. If you would like to learn more on the topic you can find some book recommendations below.\n","date":"2022-09-21","externalUrl":null,"permalink":"/posts/202209-apple-sustaining-innovation/","section":"Posts","summary":"On the 7th of September, Apple held its “Far Out” iPhone event. New iPhones, Apple Watches, and AirPods were announced, each coming with a plethora of features. Yes, all of it was innovation, but none of it brought the kind of excitement Apple used to. Why is that? And why probably this is the best strategy for Apple.","title":"Apple: iPhone 14, Watch 8, and Sustaining Innovation"},{"content":"","date":"2022-09-21","externalUrl":null,"permalink":"/tags/disruptive-innovation/","section":"Tags","summary":"","title":"Disruptive Innovation"},{"content":"","date":"2022-09-21","externalUrl":null,"permalink":"/tags/sustaining-innovation/","section":"Tags","summary":"","title":"Sustaining Innovation"},{"content":" This article was originally published externally, read the original here. Docker Hub’s Export Members functionality is now available, giving you the ability to export a full list of all your Docker users into a single CSV file. The file will contain their username, full name, and email address — as well as the user’s current status and if the user belongs to a given team. If you’re an administrator, that means you can quickly view your entire organization’s usage of Docker.\nIn the Members Tab, you can download a CSV file by pressing the Export members button. The file can be used to verify user status, confirm team structure, and quickly audit Docker usage.\nThe Export Members feature is only available for Docker Business subscribers. This feature will help organizations better track their utilization of Docker, while also simplifying the steps needed for an administrator to review their users within Docker Hub.\nAt Docker, we continually listen to our customers, and strive to build the tools needed to make them successful. Feel free to check out our public roadmap and leave feedback or requests for more features like this!\nLearn more about exporting users on our docs page, or sign in to your Docker Hub account to try it for yourself.\n","date":"2022-09-19","externalUrl":"https://www.docker.com/blog/announcing-docker-hub-export-members/","permalink":"/posts/202209-export-members/","section":"Posts","summary":"Find out how Docker Business admins can export members to track their utilization of Docker and audit Docker usage.","title":"Announcing Docker Hub Export Members"},{"content":"","date":"2022-09-04","externalUrl":null,"permalink":"/tags/amazon/","section":"Tags","summary":"","title":"Amazon"},{"content":"","date":"2022-09-04","externalUrl":null,"permalink":"/tags/disney/","section":"Tags","summary":"","title":"Disney"},{"content":"","date":"2022-09-04","externalUrl":null,"permalink":"/tags/netflix/","section":"Tags","summary":"","title":"Netflix"},{"content":"Last July and for the first time ever, streaming viewership surpassed cable. The streaming wars began around 2010 when Netflix introduced their first streaming-only plan with no DVD rentals, other players (cough, cough… Blockbuster) laughed at the time… little did they know. A decade after, there’s a multitude of streaming services available for mainstream entertainment and also for specific niches (e.g., Curiosity Stream - documentaries, Crunchyroll - anime, etc.).\nHow Did We Get Here? # This question could turn into its series of posts, aka a very long and boring answer. The short version, Netflix hit gold when they launched their streaming-only subscription service that allowed customers to watch what they wanted when they wanted. This was a massive shift from the linear TV experience where you either had to wait in front of the TV to watch your favorite show, record and watch it later, or wait for a rerun (fun fact: there was another option to set a reminder to watch your shows :D). Not only did Netflix launch a way better product to consume content, but it also changed the way users discovered new series and movies by recommending new shows based on previous history. Users flocked to this service, which was significantly better than the alternatives and also cheaper than the average cable subscription at the time (i.e., cutting the chord).\nAfter Netflix showed everyone how people really wanted to consume content, several players were interested in replicating that model themselves. Traditional content producers (e.g., HBO, Disney, etc.) saw a way to control their distribution channel and get a direct relationship with their customers. Other players, like Apple and Amazon, saw a good opportunity to expand their service subscription strategy by offering yet-another-service to their user-base. This year, most of these services reached hundreds of millions of paying users, and the competition in this industry was never been this strong.\nMassive Investment in Content # One of the strong signals of this competition is the huge amounts of money being spent in content production. HBO just released House of the Dragon, a prequel to their hit series Game of Thrones. The first season had an estimated production cost of just under $20 million per episode, making the 10 episode run cost a full $200 million. In comparison, Game of Thrones cost around $100 million per season, with the average cost per episode starting around $6 million in season 1 and going up to $15 million in the final season. This means HBO just invested roughly double what it did in the last season of Game of Thrones.\nAmazon, coincidentally, also decided to launch its prequel of a super-loved property in the fantasy drama space, Lord of the Rings. Investment in the new series, called Lord of the Rings: The Rings of Power, is estimated to be around $1 billion. Amazon bought the franchise rights to Lord of the Rings for $250 million and invested $465 million to produce the eight-episode first season of the show. In comparison, the entire trilogy of the Lord of the Rings was produced for roughly $300 million, $100 million for each movie.\nNetflix has been increasing its costs with content production in the last years, arriving at $17 billion this year. Disney is set to spend even more than that, $32 billion (even after cuts). In a different space, big tech companies are also getting into live sports. All of this spending creates a super competitive environment across all services, which is not necessarily in the best interest of the viewers.\nQuality is Decreasing # One could argue that all the investment and all the money being thrown at content production will bring the golden age of entertainment. However, as most big investments, they are made to make someone money. In the entertainment industry, one of the ways to reduce risk is to invest in properties that have already proven their value in opposition to new Intellectual Property (IP).\nThe levels of “content pumping” that we are seeing nowadays lead to a massive decrease in quality, and worse, viewer fatigue. Especially since it is focused on a minimal set of genres (i.e., fantasy, hero movies, etc.) and franchises (i.e., Marvel Cinematic Universe, DC Cinematic Universe, Star Wars, Fast and the Furious, etc.)\nMore Money In… More Money Out # When a company increases costs, usually the aim is to increase revenue and profit. Streaming services charge a fixed fee per month. Therefore, it doesn’t really matter how much or how little you loved the next big fantasy drama series from your favorite streaming provider – they will make the same amount of money from you.\nUnless… streaming companies increase subscription prices and find new ways to monetize content. Aided by the excuse of inflation, most services have increased their prices in the last year. Specifically, Disney+ subscription in the U.S. will rise from $7.99 to $10.99, while Hulu\u0026rsquo;s ad-free offering will jump from $12.99 per month to $14.99 per month. Netflix cheapest plan increased to $9.99, with the top one at $20.00. Additionally, executives from most of these big companies are also planning to roll out ads for their services. Yes, even Netflix that resisted for years cannot keep ignoring that option when faced with competition.\nUltimately, this is a worse product for the user. Price increases could make the overall cost of streaming services too high for some users, which will force a choice of which services to keep. Furthermore, you will get ads again, or you can pay more if you would rather not see them.\nWhat Happens Next? # The high levels of competition have begun to generate frustration among users. Users are starting to get fatigued with the low-quality and constant recycling of content (I swear, I am uncertain whether I can handle yet another Marvel Movie or Star Wars trilogy). The increase in prices, especially in the current economic situation, will force users to choose which services to pay for, if any. And ultimately, competition for user’s attention has never been so high. Social networks like Twitter, Instagram, and TikTok take hours of engagement daily out of each user. Video game investment has never been so high, with so many options of games and consoles.\nIn my opinion, the streaming wars have already peaked, and we’ll see a change in the upcoming years before getting to a new equilibrium. I think that one out of three things will happen:\nHorizontal Expansion - current streaming services would need to expand to provide additional services and bundle them together. This would increase the value for money and could be interesting to some users depending on what is being bundled. Netflix has begun doing this with games. Companies like Amazon and Apple came into streaming exactly for this reason, expanding into streaming even if it was not their core business. Aggregation - One service to rule them all… If one of the existing services, or a new player, can aggregate content from several players at a lower price point, this could create a captivating value proposition for customers on the lower end. This option seems highly unlikely, this was Netflix play years ago, and it didn’t work, potentially because they were greedy. Nevertheless, in the current scenario, it is highly unlikely that big players would share rights with another VS getting exclusivity on those. Merges and Acquisitions - Big fish eats smaller fish. This is bound to happen at some point, it’s just a matter of time and which company will acquire which company for what amount of money. It’s not clear if this will be good or bad for customers, depending on who starts playing together. My prediction is that ultimately, big tech companies like Amazon or Apple will begin acquiring other smaller players. These companies come into streaming as a horizontal expansion from their core businesses. Both are armed with an order of magnitude more revenues than their content only counterparts. As an example, in 2021 Disney’s revenue was $81.10 billion and Netflix\u0026rsquo;s was $24.9 billion. In comparison, Apple’s revenue in the same year was $378.32 billion and Amazon’s was $468.82 billion. With this kind of “war chest” plus their own core business (Apple devices, and Amazon e-commerce business) it will be way easier for these companies to step in when they want to.\n","date":"2022-09-04","externalUrl":null,"permalink":"/posts/202209-streaming-wars/","section":"Posts","summary":"Last July and for the first time ever, streaming viewership surpassed cable. The streaming wars began around 2010 when Netflix introduced their first streaming-only plan with no DVD rentals, other players laughed at the time… little did they know. A decade after, there’s a multitude of streaming services available for mainstream entertainment and also for specific niches. What will happen next?","title":"Streaming Wars and What Comes After… "},{"content":" This article was originally published externally, read the original here. Docker’s goal is to create a world-class product experience for our customers. We want to build a robust product that will help all teams achieve their goals. In line with that, we’ve tried to simplify the process of onboarding your team into the Docker ecosystem with our Bulk User Add feature for Docker Business and Docker Team subscriptions.\nYou can invite your team to their accounts by uploading a file including their email addresses to Docker Hub. The CSV file can either be a file you create for this specific purpose, or one that’s extracted from another in-house system. The sole requirement is that the file contains a column with the email addresses of the users that will be invited into Docker. Once the CSV file is uploaded using Docker Hub, each team member in the file will receive an invitation to use their account.\nWe’ve also updated Docker Hub’s web interface to add multiple members at once. We hope this is useful for smaller teams that can just copy and paste a list of emails directly in the web interface and onboard everyone they need. Once your team is invited, you can see both the pending and accepted invites through Docker Hub.\nBulk User Add can be used without needing to have SSO setup for your organization. This feature allows you to get the most out of your Docker Team or Business subscription, and it greatly simplifies the onboarding process.\nLearn more about the feature on our docs page, and sign in to your Docker Hub account to try it for yourself.\nAnd if you have any questions or would like to discuss this feature, please attend our upcoming\n","date":"2022-07-27","externalUrl":"https://www.docker.com/blog/bulk-user-add-for-docker-business-and-teams/","permalink":"/posts/202207-docker-bulk-add/","section":"Posts","summary":"Forget manually inviting team members to Docker Desktop. With Bulk Add for Docker Business, invite your entire team with a CSV file or their Docker IDs!","title":"Bulk User Add for Docker Business and Teams"},{"content":"","date":"2022-06-27","externalUrl":null,"permalink":"/tags/congo/","section":"Tags","summary":"","title":"Congo"},{"content":"","date":"2022-06-27","externalUrl":null,"permalink":"/categories/development/","section":"Categories","summary":"","title":"Development"},{"content":" TL;DR # Currently, there are several solutions to build and host your personal website. I wanted to challenge myself to see if I could do it with the same set of features as some of the paid solutions out there and for free. Here are the results.\nWhy would a PM build their own homepage from scratch… # A couple of months ago I decided that I wanted to start writing more in order to a) exercise my writing skills and b) get feedback on some of my ideas. With this goal in mind, I started researching tools/platforms that would help me publish without creating too much friction for me, or for the people wanting to read what I have to say e.g. having to pay. Ultimately I decided upon creating my own website.\nI had a couple of reasons for wanting to try this:\nchallenge me to see if I, a previous software engineering student, could still bodge something together that would work and feel good about not having forgotten everything I ever learned about programming,\nfind a flexible free solution that would allow me to kickstart my website without investing money right away into it, aka avoiding operational costs with platforms and services that would lock me in the future,\nhave my content hosted in a place that will not require people to pay for reading it,\nplay around with Docker\u0026rsquo;s developer environments and Microsoft\u0026rsquo;s developer containers to get myself familiar with both solutions.\nLet’s get started… # After some research, I decided to choose a website generator framework and a free hosting service. For the website framework, I choose Hugo with Congo as the theme, and for the hosting service Firebase. And for obvious reasons, I decided to set up my development environment using Docker in order to put myself in the user\u0026rsquo;s shoes for this experiment.\nI didn\u0026rsquo;t go into a deep analysis of which framework was the best for my problem as I wanted to get an MVP out fast so I went through a couple of options and picked the first one that I liked. There are several other options with different features and approaches from the one I picked. If you want to explore other options these are some you can explore: Docussaurus, Gatsby, Jekyll, Ghost, and even WordPress. The same is applicable to the hosting part, even though I chose Firebase there are other solutions like Cloudflare Pages, GitHub Pages, Digital Ocean, Netlify, Vercel, and others that you might consider exploring. If you have any suggestions for this guide feel free to reach out, always happy to chat and learn.\nTools # For this guide, I will use the following tools, which should be installed on your machine. Here is a small explanation of what each component is going to be used for and a link to the installation instructions.\nDocker - I will use Docker to configure a development environment for this project so that we can skip the need to install all the software required to run Hugo and Firebase CLI i.e. cURL, Go, Hugo, Node, NPM, etc. This will allow you to start from a git repository, start the environment and go straight into writing code instead of spending hours figuring out how to install a specific compiler for your CPU architecture. Install Docker\nVisual Studio Code - I\u0026rsquo;m using Visual Studio Code as my code editor at the moment, and all the material in the guide assumes that this is what you\u0026rsquo;re using. If you have a different preference you’ll need to adapt some parts of this guide to achieve the same results. Install Visual Studio Code\nSetup the Development Environment # Let\u0026rsquo;s start by configuring your development environment using Docker. This will allow you to create a container with all the tools you need inside of it without having to mess with your system configurations. Moreover, it will also make it easier to just delete the container and rebuild it whenever you need it instead of keeping old versions of software you don\u0026rsquo;t require daily in your personal machine.\nNote: If you just want to clone a repo with the final skeleton feel free to clone this repo and skip to the deploy section I\u0026rsquo;ll provide two ways of setting up your development environment feel free to choose the one you prefer or try both to explore the differences between them. Both options rely on a Dockerfile built by me which uses klakegg/hugo:0.93.2-ubuntu as the base image, even though this is not Hugo\u0026rsquo;s official image (since there isn\u0026rsquo;t one at the moment) it\u0026rsquo;s the one recommended on their website,.\nUsing Docker # To spin up a Dev Environment just open Docker Dashboard and select the \u0026ldquo;Dev Environments\u0026rdquo; tab on the left. If you don\u0026rsquo;t have any dev environments setup select \u0026ldquo;Create New Environment\u0026rdquo; otherwise use the button on the upper right side \u0026ldquo;Create\u0026rdquo;. Proceed to the setup step.\nHere choose the \u0026ldquo;Existing Git repo\u0026rdquo; option and use the following GitHub URL:\nhttps://github.com/nunocoracao/homepage-kickstart Note: If you clone the repo locally you can also start from the local folder Once the container is running you should see something similar to the images below.\nIn both situations, you will be able to see and click the button \u0026ldquo;Open in VSCode\u0026rdquo; which will open the editor and will allow you to start working. From there open a terminal and proceed to create the site skeleton\nUsing Visual Studio Code # Start by cloning the GitHub repository with the development environment configurations.\ngit clone https://github.com/nunocoracao/homepage-kickstart This method requires the installation of an extra VSCode extension in order to spin up the containers. Please search for Remote - Containers and install the extension to continue this guide.\nAfter successfully installing the extension, open your source folder in VSCode and open the “Remote - Containers” extension panel on the left. Select \u0026ldquo;Open Folder in Container\u0026rdquo; to spin up a container with the development environment.\nWait a couple of minutes while the image is built. Docker is creating an image with all the required software for the development of the website. This will only happen the first time you spin the environment.\nOnce the image is built, VSCode will spin the container and will place your working environment inside of it (information available in the bottom left corner of the window). You now have a development environment with Go, Hugo, Firebase CLI, and all the tools you will need for this guide. Just open a new terminal and you’re ready to begin creating your site.\n\u0026hellip;but I really want to run everything locally # If you either prefer or need to run your environment locally follow the guides below to install everything you need for your setup:\nHomebrew - Install homebrew Hugo - Install Hugo Node.js and NPM - Install node.js \u0026amp; NPM (easier to install Firebase CLI) Firebase CLI - Install Firebase CLI Create Site Skeleton # Now that we have a development environment running the first step is to create the base version of your website. For this let’s use Hugo to generate the folder template and configuration files we need by running the following command (--force parameter is required to run Hugo on a non-empty directory):\nhugo new site . --force This should have created a set of folders inside your workspace that you don’t need to worry about for now. The next step is to install a theme for Hugo. I choose Congo as it had all the features I required for my website and it seemed to be easy to change if I ever need it to. If you want to try a different theme there are several available in Hugo’s documentation, each with documentation and examples.\nInstall Congo using git submodules by running the following command:\ngit submodule add -b stable https://github.com/jpanther/congo.git themes/congo Now we need to make some changes to the directory and file structure so that Congo can work properly. I will not get into the details of what is happening in this guide (you can consult Congo’s documentation if you want to learn more), the main takeaway is that we’re creating and configuring a folder in config/_default/ which will contain all the important configuration files for Hugo and Congo.\nPlease run the following commands in order:\nmkdir -p config/_default rm config.toml cp themes/congo/config/_default/*.toml config/_default/ echo \u0026#39;theme = \u0026#34;congo\u0026#34;\u0026#39; | cat - config/_default/config.toml \u0026gt; temp \u0026amp;\u0026amp; mv temp config/_default/config.toml Congratulations, you should have your site up and running now. Let\u0026rsquo;s try it out by running Hugo\u0026rsquo;s debug server:\nhugo server -D Please open your favorite browser and navigate to localhost:1313 to see your page.\nYou should see something similar to the image above. Doesn’t look that exciting, does it? Let’s configure the theme in the next sections and learn how to create your first article.\nConfigure Theme # Now I’ll be covering how to change the look and feel of your website, add some personal information, and activate the dark mode toggle (aka the most important feature in any website these days).\nA note, I am covering a very simple configuration for this theme please check Congo’s theme documentation to understand everything you can do with this theme. Profile picture # Let’s start by adding a profile picture to your site. Create a folder called “assets” at the root of your project. Choose a profile picture and place it inside the assets folder. The rest of the guide will assume the final picture is named \u0026ldquo;profile.jpg\u0026rdquo;, so please rename your picture or take that into account when configuring some of the other files.\nIf you still need to take a proper awesome picture for this feel free to download this one to proceed with the tutorial. Configuration Files # Let’s open a couple of configuration files and start updating them. All the files we are going to change are inside config/_default/ folder.\nconfig.toml # Uncomment the baseURL parameter and replace it with the final domain of your website. This value will be used to create the robots.txt file for any search engines to successfully crawl and index your website.\nNote: if you want to configure Google Analytics please add the following line with your id to this file googleAnalytics = \u0026quot;G-XXXXXX\u0026quot; languages.en.toml # This file will drive the main information for the website and the author of the page (you). Replace the title and description for the ones you want for your page, these values will drive the HTML title and description tags.\nWithin the [author] block you can update the details that you wish to highlight in your profile. The bare minimum would be name, image, headline, and links. For the links parameter don\u0026rsquo;t forget to uncomment the last line of the file as this is a json array. Update each entry with your personal links.\nparams.toml # This file defines much of the overall behavior across the entire framework. For this tutorial I changed some of the overall values and one for the homepage, if you want to learn more about the other available configurations please consult Congo’s theme documentation.\nI\u0026rsquo;ve changed colorScheme to \u0026ldquo;ocean\u0026rdquo; which changes the global UI theme. Congo defines a three-color palette that is used throughout the theme. Valid values are congo (default), avocado, ocean, fire, and slate. Although these are the default schemes, you can also create your own. Refer to the theme\u0026rsquo;s main documentation to learn how.\nActivated showAppearanceSwitcher to enable the light/dark mode toggle. Activated enableSearch which indexes all future posts each time you build the site and provides a simple search feature. I\u0026rsquo;ve also changed the value of layout, inside [homepage], to \u0026ldquo;profile\u0026rdquo; which changes the way the landing page is rendered. Finally, the last interesting value here is showRecent, which when turned on shows the recent posts on the homepage.\nFinal # Let’s see how it looks, run the Hugo again:\nhugo server -D And navigate to localhost:1313 you should see something similar to the page below.\nCongrats it’s looking great, let’s learn how to generate your first articles.\nHow to generate articles # Hugo provides some tools to generate your articles (markdown files) with a base set of tags already in them. Run the following command to create your first post\nhugo new posts/my-first-post.md replace the contents of the file with the following:\n--- title: \u0026#34;My Published Post\u0026#34; date: 2022-06-19T20:10:29Z draft: false categories: [\u0026#34;published\u0026#34;, \u0026#34;test\u0026#34;] tags: [\u0026#34;first\u0026#34;, \u0026#34;awesome\u0026#34;] --- This is my first blog post This just created your first blog post. We\u0026rsquo;ve added a couple of categories and tags, which will be indexed by Hugo during build time. These tags will be used to create the Categories and Tags section of the website automatically for you. Notice that I\u0026rsquo;ve changed the draft to false to simulate a published post.\nRun the following command to create your second post\nhugo new posts/my-draft-post.md and replace the contents of that file with the following:\n--- title: \u0026#34;My Draft Post\u0026#34; date: 2022-06-19T20:20:39Z draft: true categories: [\u0026#34;draft\u0026#34;, \u0026#34;test\u0026#34;] tags: [\u0026#34;second\u0026#34;, \u0026#34;awesome\u0026#34;] --- This is my second blog post For the second post, I\u0026rsquo;ve left the draft parameter true to simulate a draft post.\nHugo automatically hides draft posts from the final site generation. You can keep working on articles leaving the draft label true and they will be ignored by the engine. If you want to run in DEBUG mode just use the command:\nhugo server -D If you go to the posts on the site you should be able to see both entries. If you then run the server in normal mode the draft articles will disappear. You can use the command below to do so:\nhugo server You can use this command to test the final version of the website or an article before generating the final build. When you\u0026rsquo;re ready just use the command \u0026lsquo;Hugo\u0026rsquo; to generate the final website inside the /public folder.\nhugo All files are written in Markdown which Hugo then uses to generate the final pages. I\u0026rsquo;ll not teach you how to write markdown files in this guide but I can recommend this \u0026ldquo;getting started\u0026rdquo; tutorial and this \u0026ldquo;cheat sheet\u0026rdquo; to get you started.\nDeploy # Ok, you’ve configured your website and created a couple of articles, but we still need to deploy it somewhere. As I mentioned before I chose Firebase for this guide, even though I know that it offers much more than just a simple hosting service it allows me to host my site for free without much of a hassle.\nCreate Firebase Project # Let’s start by going to https://firebase.google.com and creating an account. Once that is done you can create a project for free. The process should be straightforward and when you finish you should be in Firebase\u0026rsquo;s project dashboard.\nSetup Firebase # Now you can go back to your environment which already has Firebase CLI tools installed and ready to go. Let’s start by authenticating using:\nfirebase login Once you are successfully logged in you need to initiate the project configurations for firebase. For that please use:\nfirebase init The tool will offer you a wide variety of different options in order to configure your Firebase project. For now, we just want to configure hosting. If you are using GitHub, you might want to consider configuring GitHub action deploys which can automatically build and deploy your site every time there is a push to a specific branch or a merged pull request.\nChoose the Firebase project created before as the hosting destination. And select the configurations you wish for the deployment process. The important one here is the folder where the final files to the server will be placed and this is the public folder. For the other parameters you experiment with what better matches your use-case, the image below shows you what I picked (Note: for this tutorial I didn\u0026rsquo;t configure GitHub actions but I am using that in my real setup).\nDeploy # Ok, now for the long and boring process of deployment… joking! Once you are ready and have all your files generated by the hugo command in the public folder just use the following command to deploy:\nfirebase deploy The process should take a couple of seconds and there you go your site is deployed. The final line of the CLI tool will give you a URL to see for yourself, otherwise, you can explore your Firebase dashboard hosting section which will have more information regarding the deployment.\nConclusion # By now you should have a simple version of your website which you can configure to your needs. The main advantage of this solution is that it is flexible and extensible to a variety of different needs especially if you take the time to explore Hugo\u0026rsquo;s theme catalog. True that it might require some coding to implement complex but I would guess that this solves the problem for almost everyone.\nAbove all, it’s a completely free solution if you\u0026rsquo;re looking to start and can\u0026rsquo;t (or don\u0026rsquo;t want to) spend money. Hope this guide helps you, feel free to share it with your network and give me feedback so that I can make it better over time.\nResources # GitHub Repo for development environment GitHub Repo for base Hugo and Congo configuration GitHub Repo for base image Docker Hub image URL Hugo's Documentation Congo's Documentation Firebase Documentation Markdown Guide Markdown Getting Started\u0026quot; Markdown Cheat Sheet ","date":"2022-06-27","externalUrl":null,"permalink":"/posts/202206-homepage-guide/","section":"Posts","summary":"Currently, there are several solutions to build and host your personal website. I wanted to challenge myself to see if I could do it with the same set of features as some of the paid solutions out there and for free. Here are the results.","title":"How I Created My Homepage (for free) Using Docker, Hugo, and Firebase"},{"content":"","date":"2022-06-27","externalUrl":null,"permalink":"/tags/vscode/","section":"Tags","summary":"","title":"VSCode"},{"content":"","date":"2022-06-26","externalUrl":null,"permalink":"/tags/mentorcruise/","section":"Tags","summary":"","title":"Mentorcruise"},{"content":"","date":"2022-06-26","externalUrl":null,"permalink":"/categories/mentorship/","section":"Categories","summary":"","title":"Mentorship"},{"content":" This article was originally published externally, read the original here. I’ve been building and shipping products across multiple roles and industries, from video streaming applications to API platforms.\nWhy did you decide to become a mentor? # Throughout my career, I was fortunate enough to find (or be found by) amazing mentors that helped me in several different ways. Some provided advice for professional decisions and career management, others access to opportunities, and some just took me along for the ride to “watch \u0026amp; learn”. All of those contributed so much to make me a better professional and gave me some really interesting opportunities in my career. In the last few years, I started being a mentor myself and I am really enjoying the experience of helping others grow and fulfill their career dreams.\nWhat benefits did you gain from working with mentors early on? # The main benefit was the ability to tap into someone else’s experience and seniority and learn from their past experiences. It allowed me to get a glimpse of the future and accelerate some career milestones I wanted for myself. As an example, I never had the dream of working or living abroad, but after talking about it with my mentors (that had that experience) I learned so much about what it could bring me professionally and personally that when that opportunity came I was better prepared to make the right decision. Overall, it gave me the chance to learn how to become a better professional by watching how my mentors did it on a professional setting. Moreover, it gave me the chance to manage my career by tapping into their advice and experience in order to achieve my goals.\nHow did you get your career start? # My career started as a trainee in a telco company here in Portugal. I just left university and a junior researcher position to become a solutions architect for the TV department. It was amazing to get the first job with such responsibility and exposure to the entire organization. However, I have to say that the best part of that role was my manager who made an extra effort to coach and teach me so many invaluable lessons that I do still remember after so many years.\nGood advice is not “one-size-fits-all”\nHow do you usually set up mentorships? # I usually like to understand what are the person’s goals for the mentorship and depending on those design a specific plan. Independently of the goal, I aim to have a weekly 1:1 with each mentee to catch up on progress and see if there is anything that I can do to help with. I also like to have a log file in docs to track questions/actions/open points. What we do within this framework highly depends on the goals themselves. For example, if the mentee is looking for help in switching into a product role, I might help review the CV and LinkedIn profile and prepare them for interviews with material and mock sessions.\nWhat benefit can you provide to mentees over self-studying? # As with my own personal experience, the best benefit I can provide my mentees is to allow them to tap into my experiences, and the rational behind them, so that they can learn from it. I don’t mean to say they should do exactly as I did, no advice comes in a “one-size-fits-all” package. Moreover, I definitely made some mistakes along the way. But with that insight, each person can decide what resonates with them and what doesn’t in order to make their own decisions. In summary, I’m here to help with product topics and career management, specially people that want to get into product and don’t know where to start.\nWhat’s been your favourite mentorship story so far? # In one of my previous role I was managing two young professionals that joined the product team as QA Engineers. Both of them wanted to become Product Managers and were looking for ways to make that move happen. During one year I made an extra effort to share my experiences, give advice, and try to get them as many opportunities to have a “taste” of what PM is by getting them into meetings and brainstormings with my dev teams, shadow customer calls, attend meetings with management, etc. I was really happy the day I was able to sponsor both of them into their first PM roles still within the company. Since then it has been even more fulfilling to watch them grow and have their own successful careers in Product.\nWhat are you getting out of being a mentor? # From my experience, having access to mentors, allowed me to grow and improve at a professional and personal level faster than by myself. At this stage in my life, I feel like I’ve experienced enough to have something valuable to share with others and maybe help them achieve their goals. Ultimately, that is what I am aiming to get out of this experience, to be able to have the same impact as a mentor as my mentors had on me.\n","date":"2022-06-26","externalUrl":null,"permalink":"/posts/202206-mentorcruise-interview/","section":"Posts","summary":"Nuno Coração is one of our professional mentors on MentorCruise and works as Staff Product Manager at Docker.","title":"Nuno Coração – Meet the Mentor"},{"content":"test 1\nLorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas\n","date":"2022-06-19","externalUrl":null,"permalink":"/mentor/","section":"N9O","summary":"test 1\nLorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas Lorem ipsum alkdas","title":"Mentor"},{"content":" Experience Company Link Role Dates Location Docker Staff Product Manager 2022 - Present Full Remote Lisbon, PT AWS Startup Loft Startup Advisor 2022 - 2024 Full Remote Lisbon, PT Truphone Group Product Manager 2021 - 2022 Hybrid Lisbon, PT Senior Product Manager 2020 - 2021 Vodafone Group Analytics Squad Lead 2019 - 2020 Lisbon, PT London, UK Senior Product Manager 2018 - 2019 Sky Deutschland Senior Product Architect 2016 - 2018 Munich, DE Skimic Founder 2016 - 2017 Munich, DE Vodafone Group Product Manager 2015 - 2016 Lisbon, PTLondon, UK NOS Product Manager 2014 - 2015 Lisbon, PT Solutions Architect 2012 - 2014 Inesc-ID Junior Researcher 2011 - 2012 Lisbon, PT Education School Link Degree Date Harvard Business School Online Launching Tech Ventures 2023 Entrepreneurship Essentials 2022 Disruptive Strategy 2021 Tecnico Lisboa MSc in Computer Science - Distributed Systems 2012 BSc, Computer Science 2010 ","date":"2022-06-13","externalUrl":null,"permalink":"/resume/","section":"N9O","summary":" Experience Company Link Role Dates Location Docker Staff Product Manager 2022 - Present Full Remote Lisbon, PT AWS Startup Loft Startup Advisor 2022 - 2024 Full Remote Lisbon, PT Truphone Group Product Manager 2021 - 2022 Hybrid Lisbon, PT Senior Product Manager 2020 - 2021 Vodafone Group Analytics Squad Lead 2019 - 2020 Lisbon, PT London, UK Senior Product Manager 2018 - 2019 Sky Deutschland Senior Product Architect 2016 - 2018 Munich, DE Skimic Founder 2016 - 2017 Munich, DE Vodafone Group Product Manager 2015 - 2016 Lisbon, PTLondon, UK NOS Product Manager 2014 - 2015 Lisbon, PT Solutions Architect 2012 - 2014 Inesc-ID Junior Researcher 2011 - 2012 Lisbon, PT Education School Link Degree Date Harvard Business School Online Launching Tech Ventures 2023 Entrepreneurship Essentials 2022 Disruptive Strategy 2021 Tecnico Lisboa MSc in Computer Science - Distributed Systems 2012 BSc, Computer Science 2010 ","title":"Resume"},{"content":"I’ve been building and shipping products across multiple roles and industries, from video streaming applications to API platforms. As a product person, my main goal is to understand my users and their specific needs, in order to deliver a product that truly improves their lives.\nI’ve worked for several years in big companies and recently decided to trade that for the opportunity of working in smaller teams. Definitely prefer the latter where one can succeed/fail faster, learn faster, and overall have a way bigger impact.\nCurrently, I am focused on improving the lives of developers worldwide. I’m trying to write more about my previous experiences / random thoughts and would love to gather your feedback on it. I also love to dedicate my time to helping and mentoring other PMs or people that want to get into product.\nRecently I’ve also joined AWS Loft to help Startups and their founders with their product strategy.\nFeel free to reach out.\n","date":"2022-06-13","externalUrl":null,"permalink":"/about/","section":"N9O","summary":"I’ve been building and shipping products across multiple roles and industries, from video streaming applications to API platforms. As a product person, my main goal is to understand my users and their specific needs, in order to deliver a product that truly improves their lives.","title":"About"},{"content":"I started learning music by myself at the age of 14. I play several instruments and I love to record, compose and produce my own music. I\u0026rsquo;ve been releasing my songs under the name N9O in most streaming services. Here are the links if you want to listen.\nFollow me Tracks ","date":"2022-06-13","externalUrl":null,"permalink":"/music/","section":"N9O","summary":"I started learning music by myself at the age of 14. I play several instruments and I love to record, compose and produce my own music. I\u0026rsquo;ve been releasing my songs under the name N9O in most streaming services.","title":"Music"},{"content":"","date":"2022-06-13","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"I always try to find time to work and learn something new. Usually, most of these pet-projects don\u0026rsquo;t see the light of day. They are, however, great opportunities to try something in the real world and learn from it.\nLogo Title Description Link Blowfish A powerful, lightweight theme for Hugo built with Tailwind CSS. sitegithub Blowfish-Tools CLI to initialize a Blowfish project sitegithubNPM Wormhole A wormhole into the universe - web feed for deep space photography sitegithub ","date":"2022-06-13","externalUrl":null,"permalink":"/projects/","section":"N9O","summary":"I always try to find time to work and learn something new. Usually, most of these pet-projects don\u0026rsquo;t see the light of day. They are, however, great opportunities to try something in the real world and learn from it.","title":"Projects"},{"content":"","date":"2021-09-09","externalUrl":null,"permalink":"/tags/nintendo/","section":"Tags","summary":"","title":"Nintendo"},{"content":"","date":"2021-09-09","externalUrl":null,"permalink":"/tags/switch/","section":"Tags","summary":"","title":"Switch"},{"content":"Nintendo achieved something truly amazing with the launch of the Switch. It was able to disrupt itself and the entire gaming industry while saving itself from doom. How exactly was Nintendo able to do it and what comes next in that story?\nA Little History\u0026hellip; # Nintendo is a Japanese multinational consumer electronics and video game company with headquarters in Kyoto, Japan. In its annual report for 2021, the company reported a revenue of $16 billion (¥1,759 trillion) and it currently employs around six thousand people around the world across several different business units.\nNintendo was founded in 1889 as a company that produced and distributed hanafuda, a traditional Japanese card game. During the first half of the 1900s, the company tried to diversify into several different markets with little to no success (e.g. instant rice, love hotels, and a taxi service). During the 60s to 80s, Nintendo started investing in games, electronic toys, and gaming entertainment.\nAll of these investments culminated in the 90s with the launch of the Super Nintendo Entertainment System which sold around 50 million units worldwide and helped the company enter the US market. By then Nintendo had built several valuable assets in hardware, software, and intellectual property (including the most famous plumber that ever lived, Mario).\nAfter the Super Nintendo, the company continued to release new games and gaming devices throughout the 90s and into the 00s, including the Gameboy, Nintendo 64, GameCube, and the Wii in 2006, which contributed to make Nintendo a force to be reckoned in the gaming industry with net sales that peaked at $18 billion in 2009.\nIn 2010, the previous generation of hardware Wii was approaching the end of its life cycle and coincidentally the company’s annual net sales started dropping. At the end of 2012 the company launched its next-generation gaming console that would replace the Wii, the Wii U. However, the Wii U was a commercial failure and never got a real foothold in the console market selling less than 15 million units worldwide. The platform was described as expensive, confusing and was never able to attract support from either hardcore nor casual customers leading to Nintendo\u0026rsquo;s sales eventually plummeting to just $4 billion in 2017.\n2017 was also the same year the company was able to disrupt itself and the entire gaming industry with the launch of the Nintendo Switch which until today sold more than 89 million units worldwide, led to a reported $16 billion net sales in 2021, and ultimately contributed to save Nintendo and establish it once again as one of the biggest players in the game industry.\nThe Switch\u0026rsquo;s Disruption # The Switch entered the market as the first console that was built from the ground up to provide a hybrid experience between mobile and living room gaming (or at least the first one that was actually able to deliver that experience). This hybrid setup allowed Nintendo to create different gaming modes from connecting the Switch to a TV using a dock, to connecting the controls to the main unit and taking it to play on the go. Additionally, the actual remote of the console can be used as two separate controlling devices which allows two players to enjoy a game at once. All these different modes and combinations made the switch a super attractive console for families and casual gamers since it was an affordable and flexible option when compared to the rest of the hardware available.\nA problem that Nintendo had to solve was the fact that launching a gaming console entails an interdependency between the actual hardware and its games. Or in other words, a console is only as valuable as the catalog of games available for it. To solve this problem, Nintendo adopted an integrated strategy in order to launch the Switch with a great catalog of games focusing on the same segment that the hardware features of the console were targeting. Nintendo developed several of the initial games and leveraged its valuable intellectual property of characters and stories to sell the Switch i.e. Mario, Zelda, etc.\nThe Switch is a traditional example of a new-market disruption. Nintendo was targeting casual gamers (non-consumption for the traditional gaming industry) by offering a product that was inferior when compared to the other consoles in the market using the metrics from that time (graphics power, storage, etc) but superior when using the new set of metrics important to the new segment (fun, flexible, casual, affordable, etc). The fact that the Switch was not a super powerful device led to Sony and Microsoft not seeing Nintendo as a real competitor since their performance metrics were focused on high-end gamers and AAA titles. This created an asymmetric motivation, meaning that the incumbent companies simply conceded that market to Nintendo as it was not interesting for them. Ultimately, Nintendo gained market share with the Switch selling over 80 million units worldwide. At the moment other players still do not have the incentives to compete in that market and even if they did, they would not be able to because neither of them is competing on the same performance metrics as the Switch nor with the same organization and business structure that would allow them to succeed.\nIn July 2019, Nintendo decided to launch a cheaper version of the product called the Switch Lite, this was a clear example of the company disrupting itself. Namely, Nintendo created a low-end disruption over its own product by creating a cheaper “good enough” product that targets over-served customers of the original Switch. This created a strong foothold on the low-end market for video games which is hard to compete against.\nCurrently, Nintendo and the Switch are in a phase of sustaining innovation where incremental performance improvements in attributes are provided to the more valuable/demanding customers in the market. The proof of this is the next version of the console, due to launch in October 2021, the Nintendo Switch OLED, which is basically the same as the current Switch with a bigger screen, bigger battery, and more internal storage. This makes complete sense from a strategic point of view, after defining and deploying such a successful product, Nintendo is focusing on a deliberate strategy to grow its market share and meet the needs of its best customers in order to beat the competition, not that there is actually one at the time.\nWhat\u0026rsquo;s Next for Nintendo and the Switch? # Currently, the Switch is already the 7th best selling console of all time and the 2nd best selling handheld gaming device of all time with 89 million units sold worldwide. Considering just the consoles still in the market, the Switch has already become the number 2 device in just 4 years.\nIt is expected that, for the time being, Nintendo will retain its position in the market and keep evolving the Switch and its ecosystem with new incremental improvements. Companies like Sony and Microsoft which are targeting the high-end gamer segment will not be able to compete with Nintendo due to the humongous differences in the structures of their businesses and organization. Moreover, the other organizations also do not have any incentive to try to compete in the same market as Nintendo because, from their point of view, it is a lower margin market than the one they already have which creates an asymmetric motivation for them to flee up, conceding the low-end without fighting over it. Finally, Nintendo will have a huge advantage over new competitors that tackle its segment and it has every motivation to fight the entrance of new players into its space.\nHowever, there are still points that might require course correction to avoid potential future problems. The first one is the lack of traction from other game developers and publishers in respect to the Switch. Looking at the list of the top 10 best-selling games for the platform only 2 were not developed by Nintendo or one of its subsidiaries. The Switch developer experience has a low barrier to entry (each dev kit costing around $450) but there is a 30% “tax” on each game sold taken from the developers/publishers by Nintendo. The company could potentially look at their developer relationships and explore ways to improve the business model to get a bigger catalog of games faster. A couple of examples would be to help promote games via their channels, or the creation of an indie games program to help and promote smaller companies. Ultimately, this means that in order to grow Nintendo needs to move away from its integrated strategy into a specialized one where it focuses on the most important pieces of the system and in delivering it perfectly i.e. the console, the store, and its IP. But for that, it needs to ensure the right level of modularity so that other developers and publishers can thrive in that space.\nAnother issue with the Switch is the lack of non-gaming applications available in the device that leads to a clash with the main Job-To-Be-Done for the product, “I want entertainment for me and my family”. The Switch has the potential to be the central hub for family entertainment, however, only three video streaming applications are available on the platform: Hulu, Youtube, and Funimation. Working with companies that provide other types of entertainment like Netflix and Disney and helping them launch those services on the Switch would be a great opportunity for Nintendo to improve the feature set of the device and better serve its users.\nRegarding the future, Nintendo is clearly betting on game streaming as a way to move up in the market and disrupt yet again the other players. This would be a great technology to drive the next round of low-end disruption by offering a cheaper way to play AAA games without having to own expensive hardware and upgrade it every one or two years. However, creating its own streaming service might not be the best strategy, Nintendo should look into making its system more modular and potentially partnering with other companies like Google Stadia to get access to streaming capabilities and an existing catalog of games right away.\nUltimately, the common factor across all of Nintendo’s decisions and actions has been the ability to focus on understanding and delivering against the underlying Job-To-Be-Done for its customers. The company was able to understand that the gaming experience could solve the problem (or “job”) of family or party entertainment as well as the standard problems customers hire a gaming device to solve (i.e. play games). By organizing the entire company around these jobs, Nintendo created the ability to target non-consumption and attract a completely different segment of users to its products. Moreover, by implementing an integrated strategy that delivered new hardware, developed new games, and leveraged family-friendly characters known around the world, Nintendo was able to deliver a perfect solution for the job and completely disrupt the gaming industry. In my view, this relentless focus on the customer and how to best solve its problem is why Nintendo has become a Purpose Brand that focuses on providing great family fun and entertainment using technology.\nReferences # Henderson, Rik. “What is Nintendo Switch Cloud Streaming, how does it work and what Cloud Version games are there?” Pocket-Lint\nHerold, Charles. “10 Reasons the Wii U Was a Failure.” Lifewire.\nNintendo. “Nintendo Annual Report FY 2021.”\nOrland, Kyle. “What the “OLED Model” means for the future of Nintendo Switch.” ARS Technica.\nPeckham, Matt. “19 Things Nintendo\u0026rsquo;s President Told Us About Switch and More.” Time.\nStatista. “Nintendo\u0026rsquo;s net sales from fiscal 2008 to 2021.”\nWikipedia. “List of best-selling game consoles.”\nWikipedia. “List of best-selling Nintendo Switch video games.”\n","date":"2021-09-09","externalUrl":null,"permalink":"/posts/202109-the-future-of-nintendo/","section":"Posts","summary":"Nintendo achieved something truly amazing with the launch of the Switch. It was able to disrupt itself and the entire gaming industry while saving itself from doom. How exactly was Nintendo able to do it and what comes next in that story?","title":"The Future of the Nintendo Switch"},{"content":"","date":"1988-11-26","externalUrl":null,"permalink":"/series/the-complete-pm/","section":"Series","summary":"","title":"The Complete PM"},{"content":"bla bla\n","date":"1988-11-26","externalUrl":null,"permalink":"/posts/future-the-complete-pm/","section":"Posts","summary":"tbd","title":"The Complete Product Manager"},{"content":"","date":"1988-02-15","externalUrl":null,"permalink":"/posts/future-consistency-is-everything/","section":"Posts","summary":"","title":"Consistency is Everything - entropy and the future of work"},{"content":"","date":"1988-02-15","externalUrl":null,"permalink":"/posts/future-initial-kinetic-force/","section":"Posts","summary":"","title":"initial kinetic force"},{"content":"","date":"1988-02-15","externalUrl":null,"permalink":"/posts/future-two-mega-trends/","section":"Posts","summary":"","title":"two mega trends - AI + spacial computing"}]